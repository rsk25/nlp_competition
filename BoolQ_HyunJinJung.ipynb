{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BoolQ.ipynb","provenance":[],"authorship_tag":"ABX9TyNNzIe56+8HfP/hbpkdZ9DA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xYqodz4fUN51"},"source":["# BoolQ ("]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xxQkaZIop9_","executionInfo":{"status":"ok","timestamp":1638826096991,"user_tz":-540,"elapsed":19755,"user":{"displayName":"­정현진 / 학생 / 지능정보융합학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04939885582343227076"}},"outputId":"9b6b8fe1-5bdd-4a0f-88b5-fe068e9f780a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"ARcVg_T5ET9Y"},"source":["cur_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu0hfU4fpLQB"},"source":["### Requirement"]},{"cell_type":"code","metadata":{"id":"HjYaot57jdNQ"},"source":["!pip install transformers\n","!pip install wandb\n","!pip install pytorch-lightning\n","!pip install tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFNdyYaLpPhS"},"source":["### Import packages"]},{"cell_type":"code","metadata":{"id":"Coc0NRzFQ3aF"},"source":["import os\n","import sys\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","\n","import numpy as np\n","\n","import wandb\n","import re\n","\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLOfhdZUpikO"},"source":["### Configuration"]},{"cell_type":"code","metadata":{"id":"LSpfbqTOp1l5"},"source":["class config():\n","  \"\"\" Here type your configurations! \"\"\"\n","  # paths\n","  train_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Train.tsv\"\n","  dev_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Dev.tsv\"\n","  test_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Test.tsv\"\n","  train_dev_crop = False\n","\n","  # model\n","  model_list = {\n","      'roberta': \"xlm-roberta-large\"\n","      'bert': \"monologg/kobert\",\n","      'electra': 'monologg/koelectra-base-v3-discriminator'\n","  }\n","\n","  num_classes = 2\n","  learning_rate = 8e-6\n","\n","  # dataset\n","  k_fold = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpVbp8Dqp3tk"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"priphVDhp5ZJ"},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class BoolQ_Dataset(Dataset):\n","  def __init__(self, config, training=True):\n","    \"\"\" Configuration \"\"\" \n","    self.config = config\n","\n","    if training: # for K folding\n","      self.dataset = self.load_data(config.train_path)\n","    else: # test data\n","      self.dataset = self.load_data(config.dev_path)\n","\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","    ## Return text and label\n","    return {\n","        \"text\": self.dataset[\"text\"][idx], \n","        \"question\": self.dataset[\"question\"][idx], \n","        \"label\": self.dataset[\"label\"][idx]\n","    }\n","\n","\n","  def load_data(self, dataset_dir):\n","    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'text', 'question', 'answer'], header=0)\n","    dataset[\"label\"] = dataset[\"answer\"].astype(int)\n","    dataset['text'] = dataset['text'].apply(self.pre_process)\n","    return dataset\n","\n","  def pre_process(self, st):\n","    st = re.sub('\\(.*\\)|\\s-\\s.*', '', st)\n","    st = re.sub('\\[.*\\]|\\s-\\s.*', '', st)\n","    st = st.lower()\n","\n","    st = re.sub('[”“]', '\\\"', st)\n","    st = re.sub('[’‘]', '\\'', st)\n","    st = re.sub('[≫〉》＞』」]', '>', st)\n","    st = re.sub('[《「『〈≪＜]','<',st)\n","    st = re.sub('[−–—]', '−', st)\n","    st = re.sub('[･•・‧]','·', st)\n","    st = st.replace('／', '/')\n","    st = st.replace('℃', '도')\n","    st = st.replace('→', '에서')\n","    st = st.replace('!', '')\n","    st = st.replace('，', ',')\n","    st = st.replace('㎢', 'km')\n","    st = st.replace('∼', '~')\n","    st = st.replace('㎜', 'mm')\n","    st = st.replace('×', '곱하기')\n","    st = st.replace('=', '는')\n","    st = st.replace('®', '')\n","    st = st.replace('㎖', 'ml')\n","    st = st.replace('ℓ', 'l')\n","    st = st.replace('˚C', '도')\n","    st = st.replace('˚', '도')\n","    st = st.replace('°C', '도')\n","    st = st.replace('°', '도')\n","    st = st.replace('＋', '+')\n","    st = st.replace('*', '')\n","    st = st.replace(';', '.')\n","    return st\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YEj7z71txLF","executionInfo":{"status":"ok","timestamp":1638830071504,"user_tz":-540,"elapsed":358,"user":{"displayName":"­정현진 / 학생 / 지능정보융합학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04939885582343227076"}},"outputId":"de47fe3b-26f0-471f-8b5c-66b2ffd57fe4"},"source":["test_data = BoolQ_Dataset(config)\n","print(len(test_data))\n","\n","for data in test_data:\n","  print(data)\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3665\n","{'text': '로마 시대의 오리엔트의 범위는 제국 내에 동부 지방은 물론 제국 외부에 있는 다른 국가에 광범위하게 쓰이는 단어였다. 그 후에 로마 제국이 분열되고 서유럽이 그들의 중심적인 세계를 형성하는 과정에서 자신들을 옥시덴트, 서방이라 부르며 오리엔트는 이와 대조되는 문화를 가진 동방세계라는 뜻이 부가되어, 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어가 되었다.', 'question': '오리엔트는 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어로 쓰인다.', 'label': 1}\n"]}]},{"cell_type":"markdown","metadata":{"id":"vMs_RsrR4yxu"},"source":["### Define Model"]},{"cell_type":"code","metadata":{"id":"RZtHIWPU4ybn"},"source":["from transformers import (\n","    BertModel, \n","    BertPreTrainedModel, \n","    ElectraModel, \n","    ElectraPreTrainedModel, \n","    XLMRobertaModel, \n","    BartModel, \n","    BartPretrainedModel, \n","    T5Model, \n","    RobertaModel \n",")\n","\n","\"\"\" KoBert Pre-trained Model \"\"\"\n","\n","class Bert_BoolQ(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super(Bert, self).__init__(config)\n","        self.bert = BertModel.from_pretrained(\n","            'monologg/kobert',\n","            config=config\n","        )  # Load pretrained bert\n","        \n","        self.num_labels = config.num_labels\n","\n","        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n","        # l2 norm, similarity add\n","        self.label_classifier = FCLayer(\n","            config.hidden_size,\n","            config.num_labels,\n","            dropout_rate = 0.1, \n","            use_activation=False,\n","        )\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n","        outputs = self.bert(\n","            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]  # [CLS]\n","\n","        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n","        pooled_output = self.cls_fc_layer(pooled_output)\n","\n","        # Concat -> fc_layer\n","        logits = self.label_classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" KoElectra Pre-trained Model \"\"\"\n","\n","class Electra_BoolQ(ElectraPreTrainedModel):\n","    def __init__(self, config):\n","        super(Electra_BoolQ, self).__init__(config)\n","\n","        #self.num_labels = config.num_labels\n","        self.num_labels = config.num_labels\n","        self.model = ElectraModel.from_pretrained(\n","            'monologg/koelectra-base-v3-discriminator', config=config)\n","        self.pooling = PoolingHead(input_dim=config.hidden_size,\n","            inner_dim=config.hidden_size,\n","            pooler_dropout=0.1)\n","        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        labels=None\n","    ):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs[0][:,0,:] #cls\n","        sequence_output = self.pooling(sequence_output)\n","        logits = self.qa_classifier(sequence_output)\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" XLMRoberta Pre-trained Model \"\"\"\n","\n","class XLMRoberta_BoolQ(XLMRobertaModel):\n","    def __init__(self, config, args):\n","        super(XLMRoberta, self).__init__(config)\n","        self.xlmroberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", config=config)  # Load pretrained Electra\n","\n","        self.num_labels = config.num_labels\n","\n","        self.pooling = PoolingHead(input_dim=config.hidden_size,\n","            inner_dim=config.hidden_size,\n","            pooler_dropout=0.1)\n","        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n","        outputs = self.xlmroberta(\n","            input_ids, attention_mask=attention_mask\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[0][:, 0, :]  # [CLS]\n","\n","        pooled_output = self.pooling(pooled_output)\n","        logits = self.qa_classifier(pooled_output)\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" Additional Layers \"\"\"\n","\n","\n","class FCLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.tanh(x)\n","        return self.linear(x)\n","\n","\n","class PoolingHead(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        inner_dim: int,\n","        pooler_dropout: float,\n","    ):\n","        super().__init__()\n","        self.dense = nn.Linear(input_dim, inner_dim)\n","        self.dropout = nn.Dropout(p=pooler_dropout)\n","\n","    def forward(self, hidden_states: torch.Tensor):\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = torch.tanh(hidden_states)\n","        return hidden_states\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfYWAZVo2ag3"},"source":["### Training Center"]},{"cell_type":"code","metadata":{"id":"5301TGQkqSdC"},"source":["from transformer import AutoConfig, AutoTokenizer\n","from sklearn.model_selection import StratifiedKFold as SK\n","\n","# https://visionhong.tistory.com/30\n","# Here is the code for pl.\n","\n","class BoolQ_Model_Train():\n","  def __init__(self, config, model_name):\n","    super().__init__()\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    #####################\n","    ### Configuration ###\n","    #####################\n","\n","    \"\"\" define model \"\"\"\n","\n","    assert model_name in config.model_list.keys(), \"[Training] Give Model Name that have been listed.\"\n","\n","    # load configuration of pretrained model\n","    MODEL_CONFIG = AutoConfig.from_pretrained(config.model_list[model_name])\n","    MODEL_CONFIG.num_labels = 2\n","\n","    if model_name == \"roberta\":\n","      self.model = XLMRoberta_BoolQ(MODEL_CONFIG)\n","    elif model_name == \"bert\":\n","      self.model = Bert_BoolQ(MODEL_CONFIG)\n","    elif model_name == \"electra\":\n","      self.model = Electra_BoolQ(MODEL_CONFIG)\n","\n","    self.model.to(device)\n","\n","\n","    \"\"\" Tokenizer \"\"\"\n","\n","    self.tokenizer = AutoTokenizer.from_pretrained(config.model_list[model_name])\n","\n","\n","    \"\"\" Dataset \"\"\"\n","    # train_dataset\n","    self.train_dataset = BoolQ_Dataset(config)\n","\n","    # k_fold index\n","    skf_iris = StratifiedKFold(n_splits=config.k_fold)\n","    self.kfold = config.k_fold\n","    self.KFold_index = list(skf_iris.split(\n","        self.train_dataset.dataset['text'], self.train_dataset.dataset['label']))\n","    \n","    # batch_size\n","    self.batch_size = config.batch_size\n","\n","\n","    \"\"\" Model \"\"\"\n","\n","\n","\n","  def fit(self, epoch):\n","    for epo in tqdm(range(epoch)):\n","      ### Stratified KFold\n","      train_idx, val_idx = self.KFold_index[epo % self.kfold]\n","\n","      training_set = self.train_dataset[train_idx]\n","      validation_set = self.train_dataset[val_idx]\n","\n","      ### make dataloader\n","      train_loader = Dataloader(training_set, batch_size=64, shuffle=True, collate_fn=self.collate_fn)\n","      val_loader = Dataloader(validation_set, batch_size=64, shuffle=True, collate_fn=self.collate_fn)\n","\n","      ### train\n","      self.training_step(train_loader)\n","      \n","      \n","  def training_step(self, loader):\n","\n","\n","  # def validation_step(self):\n","\n","  # def collate_fn(self, batch):\n","\n","\n","  def collate_fn(self, batch):\n","    \n","\n","\n","\n","\n","class Model(pl.LightningModule):\n","    def __init__(self, vocab_size, Config):\n","        super().__init__()\n","        self.model = ClassifierCNN(\n","            vocab_size,\n","            Config.embed_dim,\n","            Config.hidden_dim,\n","            pretrained_embed_weight = Config._weight\n","        )\n","\n","        self.lr = Config.learning_rate\n","        self.batch_size = Config.batch_size\n","\n","    def forward(self, sentence):\n","        return self.model(sentence)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n","        return optimizer\n","\n","    def step(self, batch): # forward and calculate loss\n","        # (b, len), (b), (b), int\n","        sentence, label, scr_len, max_scr_len = self._att_tensor(batch)\n","        y_hat = self(sentence)\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(y_hat, label)\n","\n","        return loss, label, F.softmax(y_hat, dim=1)\n","\n","    def training_step(self, batch, batch_nb):\n","        loss, label, y_hat = self.step(batch)\n","        acc = (torch.argmax(y_hat, dim=1) == label).float().mean().item()\n","        tensorboard_logs = {'train_loss': loss, 'accuracy': acc}\n","        return {'loss': loss, 'accuracy': acc, 'tensorboard_log': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_nb):\n","        loss, label, y_hat = self.step(batch)\n","        return {'val_loss': loss, 'label': label.detach(), 'y_hat': y_hat.detach()}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        label = torch.cat([x['label'] for x in outputs])\n","        y_hat = torch.cat([x['y_hat'] for x in outputs])\n","        acc = (torch.argmax(y_hat, dim=1) == label).float().mean().item()\n","        print(\"Epoch {} || acc:{}\".format(self.current_epoch, acc))\n","        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': acc}\n","        return {\n","            'avg_val_loss': avg_loss,\n","            'val_acc': acc,\n","            'tensorboard_log': tensorboard_logs\n","        }\n","\n","    def train_dataloader(self):\n","        train_loader = DataLoader(\n","            Corpus_Dataset(True),\n","            batch_size = 64,\n","            shuffle = True,\n","            collate_fn = collate_fn\n","        )\n","        return train_loader\n","\n","    def val_dataloader(self):\n","        val_loader = DataLoader(\n","            Corpus_Dataset(False),\n","            batch_size = 64,\n","            shuffle = False,\n","            collate_fn = collate_fn\n","        )\n","        return val_loader\n","\n","    def _att_tensor(self, batch):\n","        device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n","        sentence_tensor = torch.tensor(batch[0]).long().to(device)\n","        label_tensor = torch.tensor(batch[1]).long().to(device)\n","        \n","        return (sentence_tensor, label_tensor, batch[2], batch[3])\n","\n"],"execution_count":null,"outputs":[]}]}