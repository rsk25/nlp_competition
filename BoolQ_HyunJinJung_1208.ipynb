{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BoolQ.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"13a2f4649e8047a8afeaf03f59d33279":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_81d37d49671547e6b5cf5f9796a7c341","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6ddc2206f25455a8fcabbdb4ca07b74","IPY_MODEL_5b074748a57e4cbab17fa772b48e58b4"]}},"81d37d49671547e6b5cf5f9796a7c341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6ddc2206f25455a8fcabbdb4ca07b74":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_view_name":"LabelView","style":"IPY_MODEL_abe820ab8fb4420b90c0cad00f1bd870","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.01MB of 0.01MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73c019de2e974d9d981fc14c7dad42f7"}},"5b074748a57e4cbab17fa772b48e58b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e73ff60e42c54fa4a436d301f1e77159","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7bf7d1a4e2b04682b2f1a08eb8b2ecd5"}},"abe820ab8fb4420b90c0cad00f1bd870":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"73c019de2e974d9d981fc14c7dad42f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e73ff60e42c54fa4a436d301f1e77159":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7bf7d1a4e2b04682b2f1a08eb8b2ecd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"xYqodz4fUN51"},"source":["# BoolQ (판정 의문문, 정현진)"]},{"cell_type":"markdown","metadata":{"id":"AIbd0ZBds4hT"},"source":["### 기본 세팅 (colab pro)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbpFe5FCspCq","executionInfo":{"status":"ok","timestamp":1638894220497,"user_tz":-540,"elapsed":13,"user":{"displayName":"Jesper Jung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06572571306595621256"}},"outputId":"17538be5-3fb7-44fc-e65a-b9c2b6f418b3"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Dec  7 16:23:40 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Q11YOefs3gY","executionInfo":{"status":"ok","timestamp":1638894223348,"user_tz":-540,"elapsed":714,"user":{"displayName":"Jesper Jung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06572571306595621256"}},"outputId":"77149f9d-0479-4f68-a07e-fd630652f901"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 27.3 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xxQkaZIop9_","executionInfo":{"status":"ok","timestamp":1638894277390,"user_tz":-540,"elapsed":3428,"user":{"displayName":"Jesper Jung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06572571306595621256"}},"outputId":"0afd5b1a-b0e9-4e6c-ed8c-5dd5e57374cb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"ARcVg_T5ET9Y"},"source":["cur_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu0hfU4fpLQB"},"source":["### Requirement"]},{"cell_type":"code","metadata":{"id":"HjYaot57jdNQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638891198730,"user_tz":-540,"elapsed":37349,"user":{"displayName":"­정현진 / 학생 / 지능정보융합학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04939885582343227076"}},"outputId":"06fc5e5d-57b6-4d7e-8c33-a97fca50e011"},"source":["!pip install transformers\n","!pip install wandb\n","!pip install pytorch-lightning\n","!pip install tqdm\n","!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 36.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 558 kB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 39.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting wandb\n","  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 4.3 MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 50.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 49.7 MB/s \n","\u001b[?25hCollecting yaspin>=1.0.0\n","  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=1f6c64eafc878c4fc277437ba986c5a05294fd0b87d5916ba74e7a3a029de9ea\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=4375db4dfa80bf47c66ea1b31544d632b531be53cfe0b39eedd74d438d05b5cd\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.5.4-py3-none-any.whl (524 kB)\n","\u001b[K     |████████████████████████████████| 524 kB 4.1 MB/s \n","\u001b[?25hCollecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 50.6 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n","Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n","\u001b[K     |████████████████████████████████| 332 kB 28.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 47.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 33.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.8)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 51.5 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 57.4 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9ebe5904578cf8ad276884888a8275f620dec34142a9dad5ee607d6a25891323\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.4 torchmetrics-0.6.1 yarl-1.7.2\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}]},{"cell_type":"markdown","metadata":{"id":"SFNdyYaLpPhS"},"source":["### Import packages"]},{"cell_type":"code","metadata":{"id":"Coc0NRzFQ3aF"},"source":["import os\n","import sys\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","\n","import numpy as np\n","\n","import wandb\n","import re\n","\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLOfhdZUpikO"},"source":["### Configuration"]},{"cell_type":"code","metadata":{"id":"LSpfbqTOp1l5"},"source":["class config():\n","  \"\"\" Here type your configurations! \"\"\"\n","  # paths\n","  train_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Train.tsv\"\n","  dev_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Dev.tsv\"\n","  test_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Test.tsv\"\n","  train_dev_crop = False\n","\n","  # model\n","  model_list = {\n","      'roberta': \"klue/roberta-large\",\n","      'bigbird': \"monologg/kobigbird-bert-base\",\n","      'electra': 'monologg/koelectra-base-v3-discriminator'\n","  }\n","\n","  num_classes = 2\n","\n","  # dataset\n","  k_fold = 5\n","  batch_size = 2\n","\n","  # optimizer, schedular\n","  learning_rate = 8e-6\n","  weight_decay = 0.01\n","  warmup_steps = 500\n","\n","  # Save\n","  log_interval = 10\n","  mode_wandb = True\n","  save_dir = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/result/\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpVbp8Dqp3tk"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"priphVDhp5ZJ"},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class BoolQ_Dataset(Dataset):\n","  def __init__(self, config, training=True):\n","    \"\"\" Configuration \"\"\" \n","    self.config = config\n","\n","    if training: # for K folding\n","      self.dataset = self.load_data(config.train_path)\n","    else: # test data\n","      self.dataset = self.load_data(config.dev_path)\n","\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","    ## Return text and label\n","    return {\n","        \"text\": self.dataset[\"text\"].values[idx], \n","        \"question\": self.dataset[\"question\"].values[idx], \n","        \"label\": self.dataset[\"label\"].values[idx]\n","    }\n","\n","\n","  def load_data(self, dataset_dir):\n","    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'text', 'question', 'answer'], header=0)\n","    dataset[\"label\"] = dataset[\"answer\"].astype(int)\n","    dataset['text'] = dataset['text'].apply(self.pre_process)\n","    return dataset\n","\n","  def pre_process(self, st):\n","    st = re.sub('\\(.*\\)|\\s-\\s.*', '', st)\n","    st = re.sub('\\[.*\\]|\\s-\\s.*', '', st)\n","    st = st.lower()\n","\n","    st = re.sub('[”“]', '\\\"', st)\n","    st = re.sub('[’‘]', '\\'', st)\n","    st = re.sub('[≫〉》＞』」]', '>', st)\n","    st = re.sub('[《「『〈≪＜]','<',st)\n","    st = re.sub('[−–—]', '−', st)\n","    st = re.sub('[･•・‧]','·', st)\n","    st = st.replace('／', '/')\n","    st = st.replace('℃', '도')\n","    st = st.replace('→', '에서')\n","    st = st.replace('!', '')\n","    st = st.replace('，', ',')\n","    st = st.replace('㎢', 'km')\n","    st = st.replace('∼', '~')\n","    st = st.replace('㎜', 'mm')\n","    st = st.replace('×', '곱하기')\n","    st = st.replace('=', '는')\n","    st = st.replace('®', '')\n","    st = st.replace('㎖', 'ml')\n","    st = st.replace('ℓ', 'l')\n","    st = st.replace('˚C', '도')\n","    st = st.replace('˚', '도')\n","    st = st.replace('°C', '도')\n","    st = st.replace('°', '도')\n","    st = st.replace('＋', '+')\n","    st = st.replace('*', '')\n","    st = st.replace(';', '.')\n","    return st\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YEj7z71txLF","executionInfo":{"status":"ok","timestamp":1638891209390,"user_tz":-540,"elapsed":1075,"user":{"displayName":"­정현진 / 학생 / 지능정보융합학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04939885582343227076"}},"outputId":"375a602b-330c-4cb8-86e7-888efccb4441"},"source":["test_data = BoolQ_Dataset(config)\n","print(len(test_data))\n","\n","for data in test_data:\n","  print(data)\n","  break\n","\n","batch = test_data[:8]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3665\n","{'text': '로마 시대의 오리엔트의 범위는 제국 내에 동부 지방은 물론 제국 외부에 있는 다른 국가에 광범위하게 쓰이는 단어였다. 그 후에 로마 제국이 분열되고 서유럽이 그들의 중심적인 세계를 형성하는 과정에서 자신들을 옥시덴트, 서방이라 부르며 오리엔트는 이와 대조되는 문화를 가진 동방세계라는 뜻이 부가되어, 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어가 되었다.', 'question': '오리엔트는 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어로 쓰인다.', 'label': 1}\n"]}]},{"cell_type":"markdown","metadata":{"id":"vMs_RsrR4yxu"},"source":["### Define Model"]},{"cell_type":"code","metadata":{"id":"RZtHIWPU4ybn"},"source":["from transformers import (\n","    BigBirdModel,\n","    BigBirdPreTrainedModel, \n","    ElectraModel, \n","    ElectraPreTrainedModel, \n","    XLMRobertaModel, \n","    BartModel, \n","    BartPretrainedModel, \n","    T5Model, \n","    RobertaModel \n",")\n","\n","\"\"\" KoBigBird Pre-trained Model \"\"\"\n","\n","class BigBird_BoolQ(BigBirdPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bigbird = BigBirdModel.from_pretrained(\n","            \"monologg/kobigbird-bert-base\",\n","            config=config\n","        )  # Load pretrained bigbird\n","        \n","        self.num_labels = config.num_labels\n","\n","        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate=0.1)\n","        # l2 norm, similarity add\n","        self.label_classifier = FCLayer(\n","            config.hidden_size,\n","            config.num_labels,\n","            dropout_rate = 0.1, \n","            use_activation=False,\n","        )\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bigbird(\n","            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]  # [CLS]\n","\n","        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n","        pooled_output = self.cls_fc_layer(pooled_output)\n","\n","        # Concat -> fc_layer\n","        logits = self.label_classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" KoElectra Pre-trained Model \"\"\"\n","\n","class Electra_BoolQ(ElectraPreTrainedModel):\n","    def __init__(self, config):\n","        super(Electra_BoolQ, self).__init__(config)\n","\n","        #self.num_labels = config.num_labels\n","        self.num_labels = config.num_labels\n","        self.model = ElectraModel.from_pretrained(\n","            'monologg/koelectra-base-v3-discriminator', config=config)\n","        self.pooling = PoolingHead(input_dim=config.hidden_size,\n","            inner_dim=config.hidden_size,\n","            pooler_dropout=0.1)\n","        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs[0][:,0,:] #cls\n","        sequence_output = self.pooling(sequence_output)\n","        logits = self.qa_classifier(sequence_output)\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" Roberta Pre-trained Model \"\"\"\n","class Roberta_BoolQ(RobertaModel):\n","    def __init__(self, config):\n","        super(Roberta, self).__init__(config)\n","        self.roberta = RobertaModel.from_pretrained(\"klue/roberta-large\", config=config)  # Load pretrained Electra\n","\n","        self.num_labels = config.num_labels\n","\n","        self.pooling = PoolingHead(input_dim=config.hidden_size,\n","            inner_dim=config.hidden_size,\n","            pooler_dropout=0.1)\n","        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.roberta(\n","            input_ids, attention_mask=attention_mask\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        pooled_output = outputs[0][:, 0, :]  # [CLS]\n","\n","        pooled_output = self.pooling(pooled_output)\n","        # pooled_output_cat = torch.cat([pooled_output, pooled_output2], dim=1)\n","        \n","        logits = self.qa_classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        return outputs  # logits, (hidden_states), (attentions)        return outputs  # logits, (hidden_states), (attentions)\n","\n","\n","\n","\"\"\" Additional Layers \"\"\"\n","\n","\n","class FCLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.tanh(x)\n","        return self.linear(x)\n","\n","\n","class PoolingHead(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        inner_dim: int,\n","        pooler_dropout: float,\n","    ):\n","        super().__init__()\n","        self.dense = nn.Linear(input_dim, inner_dim)\n","        self.dropout = nn.Dropout(p=pooler_dropout)\n","\n","    def forward(self, hidden_states: torch.Tensor):\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = torch.tanh(hidden_states)\n","        return hidden_states\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfYWAZVo2ag3"},"source":["### Training Center"]},{"cell_type":"code","metadata":{"id":"5301TGQkqSdC"},"source":["import transformers\n","from transformers import AutoConfig, AutoTokenizer\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","from torch.utils.data import Subset\n","\n","# https://visionhong.tistory.com/30\n","# Here is the code for pl.\n","\n","class BoolQ_Model_Train():\n","  def __init__(self, config, model_name):\n","    super().__init__()\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    self.device = device\n","    self.config = config\n","\n","    #####################\n","    ### Configuration ###\n","    #####################\n","\n","    \"\"\" Model \"\"\"\n","\n","    assert model_name in config.model_list.keys(), \"[Training] Please Give Correct Model Name which have been listed.\"\n","    self.model_name = model_name\n","\n","    # load configuration of pretrained model\n","    MODEL_CONFIG = AutoConfig.from_pretrained(config.model_list[model_name])\n","    MODEL_CONFIG.num_labels = 2\n","\n","    if model_name == \"roberta\":\n","      self.model = Roberta_BoolQ(MODEL_CONFIG)\n","    elif model_name == \"bigbird\":\n","      self.model = BigBird_BoolQ(MODEL_CONFIG)\n","    elif model_name == \"electra\":\n","      self.model = Electra_BoolQ(MODEL_CONFIG)\n","\n","    self.model.to(device)\n","\n","\n","    \"\"\" Tokenizer \"\"\"\n","\n","    self.tokenizer = AutoTokenizer.from_pretrained(config.model_list[model_name])\n","\n","\n","    \"\"\" Dataset \"\"\"\n","\n","    # train_dataset\n","    self.train_dataset = BoolQ_Dataset(config)\n","\n","    # k_fold index\n","    skf_iris = StratifiedKFold(n_splits=config.k_fold)\n","    self.kfold = config.k_fold\n","    self.KFold_index = list(skf_iris.split(\n","        self.train_dataset.dataset['text'], self.train_dataset.dataset['label']))\n","    \n","    # batch_size\n","    self.batch_size = config.batch_size\n","\n","\n","    \"\"\" optimizer, scheduler (in fit() function), criterion \"\"\"\n","\n","    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)\n","    self.criterion = nn.CrossEntropyLoss()\n","\n","\n","    \"\"\" Training Saving \"\"\"\n","\n","    self.log_interval = config.log_interval\n","    self.load_step = 0\n","    self.best_acc = 0\n","    self.wandb = config.mode_wandb\n","    self.save_dir = config.save_dir\n","\n","\n","\n","  def fit(self, epoch):\n","    # schedular\n","    self.scheduler = transformers.get_linear_schedule_with_warmup(\n","      self.optimizer, \n","      num_warmup_steps=config.warmup_steps, \n","      num_training_steps=len(self.train_dataset) * epoch, \n","      last_epoch= -1\n","    )\n","\n","    \n","    \"\"\" GO TRAINING. \"\"\"\n","    self.epoch = epoch\n","\n","    for epo in tqdm(range(epoch)):\n","      ### Stratified KFold\n","      train_idx, val_idx = self.KFold_index[epo % self.kfold]\n","\n","      training_set = Subset(self.train_dataset, train_idx)\n","      validation_set = Subset(self.train_dataset, val_idx)\n","\n","      ### make dataloader\n","      train_loader = DataLoader(training_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n","      val_loader = DataLoader(validation_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n","\n","      ### train\n","      self.training_step(train_loader, epo)\n","\n","      ### val\n","      self.validation_step(val_loader, epo)\n","\n","      ### Best model save\n","      if self.best_acc < self.val_acc:\n","        self.best_acc = self.val_acc\n","\n","        print(\"Best Model Saving!\")\n","\n","        model_to_save = self.model.module if hasattr(model, \"module\") else self.model\n","        model_to_save.save_pretrained(f\"{self.save_dir}/best/{self.model_name}\")\n","        torch.save(self.config, os.path.join(f\"{save_dir}/best/{self.model_name}\", \"training_config.bin\"))\n","\n","\n","      \n","      \n","\n","  def training_step(self, train_loader, epo):\n","    # allocate model to train mode\n","    self.model.train()\n","    tot_acc, tot_loss = 0., 0.\n","\n","    for texts, labels in train_loader:\n","      pbar = tqdm(total = len(train_loader), desc=\"[Training] Epoch {}\".format(epo+1))\n","      ### allocate to cuda or not.\n","      # texts -> cpu tensor, labels -> array.\n","      # texts: {input_ids, token_type_ids, attention_mask}\n","      texts = {key: torch.tensor(value).to(self.device) for key, value in texts.items()}\n","      labels = torch.tensor(labels).to(self.device)\n","\n","      ###########################################\n","      # 1) zero_grad\n","      self.optimizer.zero_grad()\n","\n","      # 2) forward\n","      y_pred = self.model(**texts)[0]\n","\n","      # 3) calculate loss\n","      loss = self.criterion(y_pred, labels)\n","\n","      # 4) backward\n","      loss.backward()\n","\n","      # 5) optimier step\n","      self.optimizer.step()\n","\n","      # 6) schedular step\n","      self.schedular.step()\n","\n","      ###########################################\n","\n","\n","      ### update, and cumulate match and loss\n","      pbar.update()\n","      self.load_step += 1\n","\n","      preds = torch.argmax(y_pred, dim=-1)\n","      tot_loss += loss.item()\n","      tot_acc += (preds == labels).sum().item() / self.batch_size\n","\n","      ### saving to log\n","      if self.load_step % self.log_interval == 0:\n","        train_loss = tot_loss / self.log_interval\n","        train_acc = tot_acc / self.log_interval\n","        current_lr = self.get_lr(self.optimizer)\n","\n","        pbar.set_description(f\"Epoch: [{epo}/{self.epochs}]({self.load_step}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n","\n","        self.train_loss = train_loss\n","        self.train_acc = train_acc\n","        self.current_lr = current_lr\n","\n","        tot_acc, tot_value = 0., 0.\n","\n","\n","\n","  def validation_step(self, val_loader, epo):\n","    # allocate model to eval mode\n","    self.model.eval()\n","    tot_acc, tot_loss = 0., 0.\n","\n","    with torch.no_grad():\n","      for texts, labels in val_loader:\n","        pbar = tqdm(total = len(val_loader), desc=\"[Validation] Epoch {}\".format(epo+1))\n","        ### allocate to cuda or not.\n","        # texts -> cpu tensor, labels -> array.\n","        # texts: {input_ids, token_type_ids, attention_mask}\n","        texts = {key: torch.tensor(value).to(self.device) for key, value in texts.items()}\n","        labels = torch.tensor(labels).to(self.device)\n","\n","        ###########################################\n","        # 1) forward\n","        y_pred = self.model(**texts)[0]\n","\n","        # 2) calculate loss\n","        loss = self.criterion(y_pred, labels)\n","\n","        ###########################################\n","        \"\"\" Update and save loss \"\"\"\n","\n","        pbar.update()\n","    \n","        preds = torch.argmax(y_pred, dim=-1)\n","        tot_loss += loss.item()\n","        tot_acc += (preds == labels).sum().item() / self.batch_size\n","\n","        ############################################\n","        \n","\n","    val_loss = tot_loss / len(val_loader)\n","    val_acc = tot_acc / len(val_loader)\n","\n","    pbar.set_description(f\"Validation: [{epo}/{self.epochs}] || loss: {val_loss:4.4} || acc: {val_acc:4.2%}\")\n","\n","    if self.wandb:\n","        wandb.log({\"train_loss\": self.train_loss, \"train_acc\": self.train_acc,\n","            \"lr\":self.current_lr, \"valid_loss\":val_loss, \"valid_acc\":val_acc\n","        })\n","\n","    self.val_acc = val_acc\n","\n","\n","\n","  def collate_fn(self, batch):\n","    \"\"\"\n","      Collate a batch of dataset to same length of text.\n","\n","    ? INPUT\n","    dataset: {text: string, question: string, label: int}\n","\n","    ? OUTPUT\n","    padded token ids.\n","    \"\"\"\n","\n","    batch_size = len(batch)\n","\n","    # integrate from dataset (dict) into list\n","    text_list = [b['text'] for b in batch]\n","    query_list = [b['question'] for b in batch]\n","    label_list = [b['label'] for b in batch]\n","    \n","    # tokenize\n","    text_query_list = list(zip(text_list, query_list))\n","\n","    if self.model_name == 'bigbird':\n","      max_length = 1024\n","    else:\n","      max_length = 512\n","\n","    tokenized_sentence = self.tokenizer(\n","        text_query_list,\n","        return_tensors=\"np\",\n","        padding=True,\n","        truncation=True,\n","        max_length=max_length,\n","        add_special_tokens=True,\n","        return_token_type_ids = True\n","    )\n","\n","    # output of tokenized_sentence: {input_ids, token_type_ids, attention_mask}\n","    return tokenized_sentence, label_list\n","\n","  def get_lr(self, optimizer):\n","    for param_group in optimizer.param_groups:\n","      return param_group['lr']\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":636,"referenced_widgets":["13a2f4649e8047a8afeaf03f59d33279","81d37d49671547e6b5cf5f9796a7c341","c6ddc2206f25455a8fcabbdb4ca07b74","5b074748a57e4cbab17fa772b48e58b4","abe820ab8fb4420b90c0cad00f1bd870","73c019de2e974d9d981fc14c7dad42f7","e73ff60e42c54fa4a436d301f1e77159","7bf7d1a4e2b04682b2f1a08eb8b2ecd5"]},"id":"sNtOL89QdxQk","executionInfo":{"status":"error","timestamp":1638891651283,"user_tz":-540,"elapsed":12563,"user":{"displayName":"­정현진 / 학생 / 지능정보융합학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04939885582343227076"}},"outputId":"eb0dd1fd-9c99-4429-f220-56ae57fb2af0"},"source":["if config.mode_wandb:\n","    wandb.login()\n","    wandb.init(project='HyunJin-BoolQ', name=\"hello\")\n","\n","Trainer = BoolQ_Model_Train(config, 'bigbird')\n","Trainer.fit(epoch = 10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"display_data","data":{"text/html":["Finishing last run (ID:29boo4zv) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 518... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13a2f4649e8047a8afeaf03f59d33279","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","</div><div class=\"wandb-col\">\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">hello</strong>: <a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/29boo4zv\" target=\"_blank\">https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/29boo4zv</a><br/>\n","Find logs at: <code>./wandb/run-20211207_153726-29boo4zv/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Successfully finished last run (ID:29boo4zv). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/2nlkh9l7\" target=\"_blank\">hello</a></strong> to <a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-8fe66a32a75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'HyunJin-BoolQ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoolQ_Model_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bigbird'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-d55df1cdde9c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, model_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectra_BoolQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.34 GiB already allocated; 9.81 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","metadata":{"id":"HTHtMZkcdrMc"},"source":["##### Test Code"]},{"cell_type":"code","metadata":{"id":"VV7rvaaI9UDE"},"source":["from transformers import AutoTokenizer, BigBirdTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n","#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n","#tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n","\n","from torch.utils.data import Subset\n","dataset = BoolQ_Dataset(config)\n","print(dataset)\n","idx = np.asarray([1, 3, 5, 6])\n","print(Subset(dataset, idx))\n","loader = DataLoader(\n","    dataset,\n","    batch_size = 8,\n","    shuffle = True,\n","    collate_fn = collate_fn\n",")\n","\n","for batch, label_list in loader:\n","  print(batch)\n","  print(batch['input_ids'].shape)\n","  print(batch['token_type_ids'].shape)\n","  print(batch['attention_mask'].shape)\n","\n","  print(tokenizer.batch_decode(batch['input_ids'].tolist()))\n","  break"],"execution_count":null,"outputs":[]}]}