{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data_dir = Path(\"./dataset/cola/\") / filename\n",
    "    dataset = pd.read_csv(\n",
    "        data_dir, \n",
    "        sep=\"\\t\", \n",
    "        header=0, \n",
    "        encoding='utf-8', \n",
    "        names=['source', 'acceptability_label', 'source_annotation', 'sentence']\n",
    "    )\n",
    "    dataset['label'] = dataset['acceptability_label'].astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_datasets(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['sentence'].tolist()\n",
    "\n",
    "    tokenize_sent = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    return tokenize_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels= None, test=False):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self, idx):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate = 0.0, use_activation=True):\n",
    "        super().__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, pooler_dropout):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p = pooler_dropout)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Electra(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size, hidden_dim=config.hidden_size, pooler_dropout=0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        return\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        # target = torch.argmax(target, dim=-1)\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "  archs = {\n",
    "    \"encoder\" : [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "    \"encoder-decoder\" : [\"T5\", \"Bart\", \"Bart_BoolQ\"]\n",
    "  }\n",
    "  for arch in archs:\n",
    "    if model_type in archs[arch]:\n",
    "      return arch\n",
    "  raise ValueError(f\"Model [{model_type}] no defined archtecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def increment_output_dir(output_path, exist_ok = False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" %path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_dir, args):\n",
    "    set_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_classes = 2\n",
    "\n",
    "    # tokenizer\n",
    "    MODEL_NAME=args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_data(\"./dataset/NIKL_CoLA_train.tsv\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad783e95c017c6a0ffc7d9d3277599f38f5f101d4b63a62b683952e68d1e005f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
