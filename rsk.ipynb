{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data_dir = Path(\"./dataset/cola/\") / filename\n",
    "    dataset = pd.read_csv(\n",
    "        data_dir, \n",
    "        sep=\"\\t\", \n",
    "        header=0, \n",
    "        encoding='utf-8', \n",
    "        names=['source', 'acceptability_label', 'source_annotation', 'sentence']\n",
    "    )\n",
    "    dataset['label'] = dataset['acceptability_label'].astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_datasets(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['sentence'].tolist()\n",
    "\n",
    "    tokenize_sent = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    return tokenize_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels= None, test=False):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate = 0.0, use_activation=True):\n",
    "        super().__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, pooler_dropout):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p = pooler_dropout)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Electra(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Electra, self).__init__(config)\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size, hidden_dim=config.hidden_size, pooler_dropout=0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        # target = torch.argmax(target, dim=-1)\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "  archs = {\n",
    "    \"encoder\" : [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "    \"encoder-decoder\" : [\"T5\", \"Bart\", \"Bart_BoolQ\"]\n",
    "  }\n",
    "  for arch in archs:\n",
    "    if model_type in archs[arch]:\n",
    "      return arch\n",
    "  raise ValueError(f\"Model [{model_type}] no defined archtecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def increment_output_dir(output_path, exist_ok = False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" %path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model_dir = args.model_dir\n",
    "    set_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_classes = 2\n",
    "\n",
    "    # tokenizer\n",
    "    MODEL_NAME=args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_data(\"./NIKL_CoLA_train.tsv\")\n",
    "    val_dataset = load_data(\"./NIKL_CoLA_dev.tsv\")\n",
    "    #test_dataset = load_data(\"./NIKL_CoLA_test.tsv\")\n",
    "\n",
    "    datasets_ = load_data(\"./NIKL_CoLA_train.tsv\")\n",
    "    labels_ = datasets_[\"label\"]\n",
    "    length = len(labels_)\n",
    "    kf = args.kfold # 1\n",
    "    class_indexs = defaultdict(list)\n",
    "    for i, label_ in enumerate(labels_):\n",
    "        class_indexs[np.argmax(label_)].append(i) #  class index [0] = [2,3,5,6], class index[1]=[나머지]\n",
    "    val_indices = set()\n",
    "    for index in class_indexs: # stratified: key : 0, 1 classindex[0][0/5:1/5]\n",
    "        val_indices = (val_indices | set(class_indexs[index][int(len(class_indexs[index])*(kf-1)/9) : int(len(class_indexs[index])*kf/9)]))\n",
    "    train_indices = set(range(length)) - val_indices\n",
    "\n",
    "    train_dataset = datasets_.loc[np.array(list(train_indices))]\n",
    "    val_dataset = datasets_.loc[np.array(list(val_indices))]\n",
    "\n",
    "    train_label = train_dataset['label'].values\n",
    "    val_label = val_dataset['label'].values\n",
    "\n",
    "    tokenized_train = tokenize_datasets(train_dataset, tokenizer, check_arch(args.model_type))\n",
    "    tokenized_val = tokenize_datasets(val_dataset, tokenizer, check_arch(args.model_type))\n",
    "\n",
    "    train_dataset = ColaDataset(tokenized_train, train_label)\n",
    "    val_dataset = ColaDataset(tokenized_val, val_label)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if args.model_type == 'Electra_BoolQ':\n",
    "        config_module = getattr(import_module(\"transformers\"), \"ElectraConfig\")\n",
    "    else:\n",
    "        config_module = getattr(import_module(\"transformers\"), args.model_type + \"Config\")\n",
    "    \n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model = Electra.from_pretrained(MODEL_NAME, config=model_config)\n",
    "\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    save_dir = increment_output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if ('cls_fc_layer' not in name) and ('label_classifier' not in name): # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion = create_criterion(args.criterion)  # default: cross_entropy\n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps = 1e-8\n",
    "    )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=args.warmup_steps, \n",
    "        num_training_steps=len(train_loader) * args.epochs, \n",
    "        last_epoch=- 1\n",
    "    )   \n",
    "\n",
    "    ## logging\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_mcc = -1\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(train_loader, dynamic_ncols=True)\n",
    "        # train loop\n",
    "        # unFreeze parameters\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item['labels'])\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item['labels']).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                pbar.set_description(f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0     \n",
    "\n",
    "    ## validation\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, dynamic_ncols=True)\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        acc_okay = 0\n",
    "        count_all = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        eps = 1e-9\n",
    "        for idx, items in enumerate(pbar):\n",
    "            sleep(0.01)\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            outs = model(**item)\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "            loss = criterion(outs[0], item['labels']).item()\n",
    "\n",
    "            acc_item = (item['labels'] == preds).sum().item()\n",
    "\n",
    "            TRUE = (item['labels'] == preds)\n",
    "            FALSE = (item['labels'] != preds)\n",
    "\n",
    "            TP += (TRUE * preds).sum().item()\n",
    "            TN += (TRUE * (preds==0)).sum().item()\n",
    "            FP += (FALSE * preds).sum().item()\n",
    "            FN += (FALSE * (preds==0)).sum().item()\n",
    "\n",
    "            val_loss_items.append(loss)\n",
    "            val_acc_items.append(acc_item)\n",
    "            acc_okay += acc_item\n",
    "            count_all += len(preds)\n",
    "\n",
    "            MCC = ((TP*TN) - (FP*FN)) / (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "\n",
    "            pbar.set_description(f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(val_loader)}) || val_loss: {loss:4.4} || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "        val_acc = acc_okay / count_all\n",
    "\n",
    "        if MCC > best_val_mcc:\n",
    "            print(f\"New best model for val mcc : {MCC:4.2%}! saving the best model..\")\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "            torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "            best_val_mcc = MCC\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "            f\"best mcc : {best_val_mcc:4.2%}, best loss: {best_val_loss:4.4}|| \"\n",
    "            f\"MCC : {MCC:4.2%}|| \"\n",
    "            f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\"\n",
    "        )\n",
    "\n",
    "        if args.wandb:\n",
    "            wandb.log({\"train_loss\": train_loss, \"train_acc\":train_acc,\n",
    "                \"lr\":current_lr, \"valid_loss\":val_loss, \"valid_acc\":val_acc, \n",
    "                \"MCC\":MCC, \"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN})\n",
    "        print(f'Time elapsed: {(time.time() - start_time)/60: .2f} min',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "k-fold num : 9\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing Electra: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing Electra from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Electra from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Electra were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['pooling.dense.bias', 'classifier.bias', 'classifier.weight', 'pooling.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: [0/30](440/441) || loss: 0.5727 || acc: 70.16% || lr 3.52e-06: 100%|██████████| 441/441 [00:22<00:00, 20.02it/s]\n",
      "Epoch: [1/30](440/441) || loss: 0.5048 || acc: 75.62% || lr 3.88e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [2/30](440/441) || loss: 0.4682 || acc: 79.22% || lr 3.742e-06: 100%|██████████| 441/441 [00:21<00:00, 20.22it/s]\n",
      "Epoch: [3/30](440/441) || loss: 0.4017 || acc: 82.03% || lr 3.603e-06: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [4/30](440/441) || loss: 0.3513 || acc: 85.62% || lr 3.465e-06: 100%|██████████| 441/441 [00:21<00:00, 20.27it/s]\n",
      "Epoch: [5/30](440/441) || loss: 0.3357 || acc: 86.72% || lr 3.326e-06: 100%|██████████| 441/441 [00:21<00:00, 20.26it/s]\n",
      "Epoch: [6/30](440/441) || loss: 0.3224 || acc: 86.56% || lr 3.187e-06: 100%|██████████| 441/441 [00:21<00:00, 20.27it/s]\n",
      "Epoch: [7/30](440/441) || loss: 0.2674 || acc: 89.69% || lr 3.049e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [8/30](440/441) || loss: 0.1985 || acc: 92.81% || lr 2.91e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [9/30](440/441) || loss: 0.2158 || acc: 92.03% || lr 2.772e-06: 100%|██████████| 441/441 [00:21<00:00, 20.31it/s]\n",
      "Epoch: [10/30](440/441) || loss: 0.1797 || acc: 94.22% || lr 2.633e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [11/30](440/441) || loss: 0.2114 || acc: 92.19% || lr 2.495e-06: 100%|██████████| 441/441 [00:21<00:00, 20.30it/s]\n",
      "Epoch: [12/30](440/441) || loss: 0.15 || acc: 94.69% || lr 2.356e-06: 100%|██████████| 441/441 [00:21<00:00, 20.30it/s]\n",
      "Epoch: [13/30](440/441) || loss: 0.1617 || acc: 93.75% || lr 2.217e-06: 100%|██████████| 441/441 [00:21<00:00, 20.23it/s]\n",
      "Epoch: [14/30](440/441) || loss: 0.1577 || acc: 94.53% || lr 2.079e-06: 100%|██████████| 441/441 [00:21<00:00, 20.31it/s]\n",
      "Epoch: [15/30](440/441) || loss: 0.1767 || acc: 92.97% || lr 1.94e-06: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [16/30](440/441) || loss: 0.1367 || acc: 95.47% || lr 1.802e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [17/30](440/441) || loss: 0.109 || acc: 97.03% || lr 1.663e-06: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [18/30](440/441) || loss: 0.1292 || acc: 95.47% || lr 1.525e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [19/30](440/441) || loss: 0.1444 || acc: 94.53% || lr 1.386e-06: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [20/30](440/441) || loss: 0.1178 || acc: 96.25% || lr 1.247e-06: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [21/30](440/441) || loss: 0.1168 || acc: 96.09% || lr 1.109e-06: 100%|██████████| 441/441 [00:21<00:00, 20.27it/s]\n",
      "Epoch: [22/30](440/441) || loss: 0.08918 || acc: 96.41% || lr 9.703e-07: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [23/30](440/441) || loss: 0.09208 || acc: 97.34% || lr 8.317e-07: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [24/30](440/441) || loss: 0.06299 || acc: 97.97% || lr 6.932e-07: 100%|██████████| 441/441 [00:21<00:00, 20.25it/s]\n",
      "Epoch: [25/30](440/441) || loss: 0.1111 || acc: 96.72% || lr 5.546e-07: 100%|██████████| 441/441 [00:21<00:00, 20.29it/s]\n",
      "Epoch: [26/30](440/441) || loss: 0.05485 || acc: 98.12% || lr 4.16e-07: 100%|██████████| 441/441 [00:21<00:00, 20.23it/s]\n",
      "Epoch: [27/30](440/441) || loss: 0.06228 || acc: 97.34% || lr 2.775e-07: 100%|██████████| 441/441 [00:21<00:00, 20.28it/s]\n",
      "Epoch: [28/30](440/441) || loss: 0.08842 || acc: 97.34% || lr 1.389e-07: 100%|██████████| 441/441 [00:21<00:00, 20.26it/s]\n",
      "Epoch: [29/30](440/441) || loss: 0.07106 || acc: 97.81% || lr 3.142e-10: 100%|██████████| 441/441 [00:21<00:00, 20.27it/s]\n",
      "Epoch: [29/30](3/14) || val_loss: 1.161 || acc: 81.25% || MCC: 62.63%:  14%|█▍        | 2/14 [00:00<00:00, 18.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [29/30](14/14) || val_loss: 0.8798 || acc: 80.27% || MCC: 60.98%: 100%|██████████| 14/14 [00:00<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val mcc : 60.98%! saving the best model..\n",
      "[Val] acc : 80.27%, loss: 0.8757|| best mcc : 60.98%, best loss: 0.8757|| MCC : 60.98%|| TP:773 / TN:643 / FP:230 / FN:118\n",
      "Time elapsed:  10.90 min \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "args = argparse.Namespace(\n",
    "    seed = 42,\n",
    "    epochs = 30,\n",
    "    freeze_epoch=0,\n",
    "    optimizer = 'AdamW',\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = 500,\n",
    "    log_interval = 20,\n",
    "    kfold = 9,\n",
    "\n",
    "    criterion = 'cross_entropy',\n",
    "    dropout_rate = 0.2,\n",
    "    model_type = \"Electra\",\n",
    "    pretrained_model = \"monologg/koelectra-base-v3-discriminator\",\n",
    "    lr = 4e-6,\n",
    "    batch_size = 32,\n",
    "    valid_batch_size = 128,\n",
    "\n",
    "    val_ratio=0.2,\n",
    "    name = 'exp',\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR', './results'),\n",
    "    wandb = False,\n",
    "    custompretrain = \"\",\n",
    ")\n",
    "\n",
    "print('='*40)\n",
    "print(f\"k-fold num : {args.kfold}\")\n",
    "print('='*40)\n",
    "\n",
    "args.name = f'{args.model_type}V3_{args.lr}_9k{args.kfold}'\n",
    "\n",
    "if args.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    wandb.init(project='NIKL-COLA', name=args.name, config=vars(args))\n",
    "\n",
    "train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad783e95c017c6a0ffc7d9d3277599f38f5f101d4b63a62b683952e68d1e005f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
