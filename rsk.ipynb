{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoLA (문법성 판단)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import re\n",
    "\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch additional data\n",
    "추가 학습을 위해 KorSTS와 KorNLI 데이터셋에서 텍스트만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가적으로 사용할 추가 데이터 가져오기\n",
    "def create_additional_text(name):\n",
    "    path = Path('.')\n",
    "    root_dir = path / 'dataset'\n",
    "    Korpora.fetch(name, root_dir = root_dir)\n",
    "    corpus = Korpora.load(name)\n",
    "    additional_text = corpus.get_all_texts()\n",
    "\n",
    "    temp = []\n",
    "    for i, text in enumerate(tqdm(additional_text)):\n",
    "        temp.append(text+'\\n')\n",
    "\n",
    "    additional_text = set(temp)\n",
    "    del temp\n",
    "    additional_text = list(additional_text)\n",
    "    \n",
    "    new_file = root_dir / 'additional.txt'\n",
    "    with open(new_file, 'a+', encoding='utf-8') as writer:\n",
    "        writer.writelines(additional_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_additional_text(\"korsts\")\n",
    "#create_additional_text(\"kornli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. 추가 데이터셋에서 일부러 조사가 틀린 텍스트를 만들어서 문법이 틀린 데이터도 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyjosa import josa, jonsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SabotageSentence(object):\n",
    "    def __init__(self, sentence: str):\n",
    "        self.sentence = sentence\n",
    "        self.josa_dict = {\n",
    "            'for_jongsung':['을','은','이','과'], \n",
    "            'no_jongsung':['를','는','가','와','나','로','야','랑','며']\n",
    "        }\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_all_josa(self):\n",
    "        return self.josa_dict\n",
    "\n",
    "\n",
    "    def jongsung_wrong_josa(self) -> str:\n",
    "        new_sent = ''\n",
    "        for _, word in enumerate(self.sentence.split()):\n",
    "            tmp_word = word[:-1]\n",
    "            if word[-1] in self.josa_dict['for_jongsung']:\n",
    "                tmp_word+=random.choice(self.josa_dict['no_jongsung'])\n",
    "                new_sent+=tmp_word\n",
    "                new_sent+=' '\n",
    "            elif word[-1] in self.josa_dict['no_jongsung']:\n",
    "                tmp_word+=random.choice(self.josa_dict['for_jongsung'])\n",
    "                new_sent+=tmp_word\n",
    "                new_sent+=' '\n",
    "            else:\n",
    "                new_sent+=word\n",
    "                new_sent+=' '\n",
    "\n",
    "        return new_sent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 기존 데이터셋을 pandas DataFrame으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data_dir = Path(\"./dataset/cola/\") / filename\n",
    "    dataset = pd.read_csv(\n",
    "        data_dir, \n",
    "        sep=\"\\t\", \n",
    "        header=0, \n",
    "        encoding='utf-8', \n",
    "        names=['source', 'acceptability_label', 'source_annotation', 'sentence']\n",
    "    )\n",
    "    dataset['label'] = dataset['acceptability_label'].astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def augment_data_orig(new_data: List[str]):\n",
    "    tmp_data_holder = {'source':[], 'label':[], 'source_annotation':[], 'sentence':[]}\n",
    "    for i, row in enumerate(new_data):\n",
    "        if (re.match('[a-zA-Z]', row) is not None) or (len(row) >= 70) or (len(row) == 0) or (row[-2:]!='.\\n'):\n",
    "            continue\n",
    "        else:\n",
    "            tmp_data_holder['source'].append('T'+str(10001+i))\n",
    "            tmp_data_holder['label'].append(1)\n",
    "            tmp_data_holder['source_annotation'].append('*')\n",
    "            assert type(row) == str\n",
    "            tmp_data_holder['sentence'].append(row.replace('\\n',''))\n",
    "\n",
    "    dataset = pd.DataFrame(tmp_data_holder)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def augment_data(data):\n",
    "    tmp_data_holder = {'source':[], 'label':[], 'source_annotation':[], 'sentence':[]}\n",
    "    for _, row in data.iterrows():\n",
    "        tmp_data_holder['source'].append(row['source'])\n",
    "        tmp_data_holder['label'].append(0)\n",
    "        tmp_data_holder['source_annotation'].append(np.NaN)\n",
    "        \n",
    "        text = SabotageSentence(row['sentence']).jongsung_wrong_josa()\n",
    "        tmp_data_holder['sentence'].append(text)\n",
    "\n",
    "    dataset = pd.DataFrame(tmp_data_holder)\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "def read_txt(path='./dataset/additional.txt') -> List[str]:\n",
    "    with open(path, 'r+', encoding='utf-8') as reader:\n",
    "        new_data = reader.readlines()\n",
    "    \n",
    "    tmp_list = []\n",
    "    for text in new_data:\n",
    "        text.rstrip('\\n')\n",
    "        text.replace('\\n','')\n",
    "        tmp_list.append(text)\n",
    "    new_data = tmp_list\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def multiprocess_aug(orig_dataset, func_name):\n",
    "    num_process = multiprocessing.cpu_count()\n",
    "    \n",
    "    chunk_size = int(orig_dataset.shape[0] / num_process)\n",
    "    chunks = [orig_dataset.iloc[orig_dataset.index[i:i+chunk_size]] for i in range(0, orig_dataset.shape[0], chunk_size)]\n",
    "    assert len(chunks) != 0\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_process) as pool:\n",
    "        results = pool.map(func_name, chunks)\n",
    "        \n",
    "        new_dataset = pd.concat(results)\n",
    "        dataset = pd.concat([orig_dataset, new_dataset])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_datasets(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['sentence'].tolist()\n",
    "    tokenize_sent = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 200,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    return tokenize_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels= None, test=False):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Electra(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Electra, self).__init__(config)\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        output = outputs[0][:, 0, :]\n",
    "        output = self.linear(self.dropout(output))\n",
    "        output = torch.tanh(output)\n",
    "        logits = self.classifier(output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "  archs = {\n",
    "    \"encoder\" : [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "    \"encoder-decoder\" : [\"T5\", \"Bart\", \"Bart_BoolQ\"]\n",
    "  }\n",
    "  for arch in archs:\n",
    "    if model_type in archs[arch]:\n",
    "      return arch\n",
    "  raise ValueError(f\"Model [{model_type}] no defined archtecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def output_dir(output_path, exist_ok = False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" %path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model_dir = args.model_dir\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # tokenizer\n",
    "    MODEL_NAME = args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    datasets_ = load_data(\"./NIKL_CoLA_train.tsv\")\n",
    "\n",
    "    # 아래 코드는 데이터를 증강하는 코드지만 MCC에 도움이 되지 않는 관계로 주석처리함\n",
    "    # new_data = read_txt()\n",
    "    # new_data = augment_data_orig(new_data)\n",
    "    # new_data_corrupt = multiprocess_aug(new_data, augment_data)\n",
    "    # datasets_ = pd.concat([datasets_, new_data, new_data_corrupt], ignore_index=True)\n",
    "\n",
    "    # make validation sets from training set\n",
    "    labels_ = datasets_[\"label\"]\n",
    "    length = len(labels_)\n",
    "    kf = args.kfold\n",
    "    class_indexs = defaultdict(list)\n",
    "    for i, label_ in enumerate(labels_):\n",
    "        class_indexs[np.argmax(label_)].append(i)\n",
    "    val_indices = set()\n",
    "    for index in class_indexs: \n",
    "        val_indices = (val_indices | set(class_indexs[index][int(\n",
    "            len(class_indexs[index])*(kf-1)/9): int(len(class_indexs[index])*kf/9)]))\n",
    "    train_indices = set(range(length)) - val_indices\n",
    "\n",
    "    train_dataset = datasets_.loc[np.array(list(train_indices))]\n",
    "    val_dataset = datasets_.loc[np.array(list(val_indices))]\n",
    "\n",
    "    train_label = train_dataset['label'].values\n",
    "    val_label = val_dataset['label'].values\n",
    "\n",
    "    tokenized_train = tokenize_datasets(\n",
    "        train_dataset, tokenizer, check_arch(args.model_type))\n",
    "    tokenized_val = tokenize_datasets(\n",
    "        val_dataset, tokenizer, check_arch(args.model_type))\n",
    "\n",
    "    train_dataset = ColaDataset(tokenized_train, train_label)\n",
    "    val_dataset = ColaDataset(tokenized_val, val_label)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    config_module = getattr(import_module(\n",
    "        \"transformers\"), args.model_type + \"Config\")\n",
    "\n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model = Electra.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    save_dir = output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if ('cls_fc_layer' not in name) and ('label_classifier' not in name):  # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion = create_criterion(args.criterion)  # default: cross_entropy\n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=len(train_loader) * args.epochs,\n",
    "        last_epoch=- 1\n",
    "    )\n",
    "\n",
    "    # logging\n",
    "    best_val_mcc = -1\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(train_loader, dynamic_ncols=True)\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item['labels'])\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item['labels']).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                pbar.set_description(\n",
    "                    f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, dynamic_ncols=True)\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        acc_okay = 0\n",
    "        count_all = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        eps = 1e-9\n",
    "        for idx, items in enumerate(pbar):\n",
    "            sleep(0.01)\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            outs = model(**item)\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "            loss = criterion(outs[0], item['labels']).item()\n",
    "\n",
    "            acc_item = (item['labels'] == preds).sum().item()\n",
    "\n",
    "            TRUE = (item['labels'] == preds)\n",
    "            FALSE = (item['labels'] != preds)\n",
    "\n",
    "            TP += (TRUE * preds).sum().item()\n",
    "            TN += (TRUE * (preds == 0)).sum().item()\n",
    "            FP += (FALSE * preds).sum().item()\n",
    "            FN += (FALSE * (preds == 0)).sum().item()\n",
    "\n",
    "            val_loss_items.append(loss)\n",
    "            val_acc_items.append(acc_item)\n",
    "            acc_okay += acc_item\n",
    "            count_all += len(preds)\n",
    "\n",
    "            # Calculate MCC\n",
    "            MCC = ((TP*TN) - (FP*FN)) / \\\n",
    "                (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(val_loader)}) || val_loss: {loss:4.4} || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "        val_acc = acc_okay / count_all\n",
    "\n",
    "        if MCC > best_val_mcc:\n",
    "            print(\n",
    "                f\"New best model for val mcc : {MCC:4.2%}! saving the best model..\")\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "            torch.save(args, os.path.join(\n",
    "                f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "            best_val_mcc = MCC\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "            f\"best mcc : {best_val_mcc:4.2%}, best loss: {best_val_loss:4.4}|| \"\n",
    "            f\"MCC : {MCC:4.2%}|| \"\n",
    "            f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\"\n",
    "        )\n",
    "\n",
    "    time.sleep(5)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "args = argparse.Namespace(\n",
    "    seed = 42,\n",
    "    epochs = 30,\n",
    "    freeze_epoch=0,\n",
    "    optimizer = 'AdamW',\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = 500,\n",
    "    log_interval = 20,\n",
    "    kfold = 9,\n",
    "\n",
    "    criterion = 'cross_entropy',\n",
    "    dropout_rate = 0.1,\n",
    "    model_type = \"Electra\",\n",
    "    pretrained_model = \"tunib/electra-ko-base\",\n",
    "    lr = 4e-6,\n",
    "    batch_size = 32,\n",
    "    valid_batch_size = 128,\n",
    "\n",
    "    val_ratio=0.2,\n",
    "    name = 'exp',\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR', './results'),\n",
    "    custompretrain = \"\"\n",
    ")\n",
    "\n",
    "args.name = f'{args.model_type}_{args.lr}_{args.kfold}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "k-fold num : 9\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tunib/electra-ko-base were not used when initializing Electra: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing Electra from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Electra from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Electra were not initialized from the model checkpoint at tunib/electra-ko-base and are newly initialized: ['classifier.bias', 'linear.bias', 'classifier.weight', 'linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: [0/30](440/441) || loss: 0.5605 || acc: 74.53% || lr 3.52e-06: 100%|██████████| 441/441 [01:57<00:00,  3.74it/s]\n",
      "Epoch: [1/30](440/441) || loss: 0.4917 || acc: 78.44% || lr 3.88e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [2/30](440/441) || loss: 0.4534 || acc: 80.31% || lr 3.742e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [3/30](440/441) || loss: 0.4011 || acc: 82.66% || lr 3.603e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [4/30](440/441) || loss: 0.3474 || acc: 86.41% || lr 3.465e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [5/30](440/441) || loss: 0.3023 || acc: 87.19% || lr 3.326e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [6/30](440/441) || loss: 0.3168 || acc: 88.12% || lr 3.187e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [7/30](440/441) || loss: 0.2279 || acc: 91.41% || lr 3.049e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [8/30](440/441) || loss: 0.258 || acc: 89.53% || lr 2.91e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [9/30](440/441) || loss: 0.1729 || acc: 92.50% || lr 2.772e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [10/30](440/441) || loss: 0.1754 || acc: 92.03% || lr 2.633e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [11/30](440/441) || loss: 0.16 || acc: 93.91% || lr 2.495e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [12/30](440/441) || loss: 0.1762 || acc: 94.84% || lr 2.356e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [13/30](440/441) || loss: 0.1419 || acc: 95.00% || lr 2.217e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [14/30](440/441) || loss: 0.1195 || acc: 95.62% || lr 2.079e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [15/30](440/441) || loss: 0.1356 || acc: 94.84% || lr 1.94e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [16/30](440/441) || loss: 0.1306 || acc: 95.94% || lr 1.802e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [17/30](440/441) || loss: 0.08656 || acc: 96.72% || lr 1.663e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [18/30](440/441) || loss: 0.08457 || acc: 96.41% || lr 1.525e-06: 100%|██████████| 441/441 [01:51<00:00,  3.97it/s]\n",
      "Epoch: [19/30](440/441) || loss: 0.09354 || acc: 96.56% || lr 1.386e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [20/30](440/441) || loss: 0.07041 || acc: 97.66% || lr 1.247e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [21/30](440/441) || loss: 0.08624 || acc: 96.41% || lr 1.109e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [22/30](440/441) || loss: 0.06864 || acc: 97.50% || lr 9.703e-07: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [23/30](440/441) || loss: 0.05669 || acc: 98.12% || lr 8.317e-07: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [24/30](440/441) || loss: 0.06478 || acc: 97.97% || lr 6.932e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [25/30](440/441) || loss: 0.09108 || acc: 97.03% || lr 5.546e-07: 100%|██████████| 441/441 [01:52<00:00,  3.93it/s]\n",
      "Epoch: [26/30](440/441) || loss: 0.04714 || acc: 98.59% || lr 4.16e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [27/30](440/441) || loss: 0.04634 || acc: 98.12% || lr 2.775e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [28/30](440/441) || loss: 0.06097 || acc: 98.28% || lr 1.389e-07: 100%|██████████| 441/441 [01:52<00:00,  3.93it/s]\n",
      "Epoch: [29/30](440/441) || loss: 0.05658 || acc: 97.97% || lr 3.142e-10: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [29/30](1/14) || val_loss: 0.8176 || acc: 83.59% || MCC: 67.18%:   7%|▋         | 1/14 [00:00<00:01,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [29/30](14/14) || val_loss: 0.7995 || acc: 80.67% || MCC: 61.83%: 100%|██████████| 14/14 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val mcc : 61.83%! saving the best model..\n",
      "[Val] acc : 80.67%, loss: 0.8838|| best mcc : 61.83%, best loss: 0.8838|| MCC : 61.83%|| TP:779 / TN:644 / FP:229 / FN:112\n"
     ]
    }
   ],
   "source": [
    "print('='*40)\n",
    "print(f\"k-fold num : {args.kfold}\")\n",
    "print('='*40)\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "    file = 'NIKL_CoLA_dev.tsv'\n",
    "    dataset = load_data(file)\n",
    "    tokenized_test = tokenize_datasets(dataset, tokenizer)\n",
    "    test_label = dataset['label'].values\n",
    "    test_dataset = ColaDataset(tokenized_test, test_label)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = Electra.from_pretrained(args.model_dir) \n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(test_loader)\n",
    "    print(\"Calculating validation results...\")\n",
    "    test_acc_items = []\n",
    "    acc_okay = 0\n",
    "    count_all = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    eps = 1e-9\n",
    "\n",
    "    for idx, items in enumerate(pbar):\n",
    "        sleep(0.01)\n",
    "\n",
    "        item = {key: val.to(device) for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outs = model(**item)\n",
    "\n",
    "        preds = torch.argmax(outs[0], dim=-1)\n",
    "        labels = item['labels']\n",
    "\n",
    "        acc_item = (labels == preds).sum().item()\n",
    "\n",
    "        TRUE = (labels == preds)\n",
    "        FALSE = (labels != preds)\n",
    "\n",
    "        TP += (TRUE * preds).sum().item()\n",
    "        TN += (TRUE * (preds==0)).sum().item()\n",
    "        FP += (FALSE * preds).sum().item()\n",
    "        FN += (FALSE * (preds==0)).sum().item()\n",
    "\n",
    "        MCC = ((TP*TN) - (FP*FN)) / (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "\n",
    "        test_acc_items.append(acc_item)\n",
    "        acc_okay += acc_item\n",
    "        count_all += len(preds)\n",
    "\n",
    "        pbar.set_description(f\"({idx + 1}/{len(test_loader)}) || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "    test_acc = acc_okay / count_all\n",
    "\n",
    "    print(\n",
    "        f\"[Test] acc : {test_acc:4.2%}|| \"\n",
    "        f\"MCC : {MCC:4.2%}|| \"\n",
    "        f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\\n\"\n",
    "        f\"======================================\\n\"\n",
    "        f\"Test MCC: {MCC:4.2%}\"\n",
    "    )\n",
    "    time.sleep(5)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval args\n",
    "args = argparse.Namespace(\n",
    "    model_type = \"Electra\",\n",
    "    pretrained_model = \"tunib/electra-ko-base\",\n",
    "\n",
    "    model_dir = './results/Electra_4e-06_9/97/best',\n",
    "    criterion = 'cross_entropy',\n",
    "    num_labels=2,\n",
    "\n",
    "    test_batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(9/254) || acc: 76.39% || MCC: 53.29%:   2%|▏         | 5/254 [00:00<00:05, 49.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(254/254) || acc: 75.84% || MCC: 51.64%: 100%|██████████| 254/254 [00:05<00:00, 48.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] acc : 75.84%|| MCC : 51.64%|| TP:892 / TN:649 / FP:312 / FN:179\n",
      "======================================\n",
      "Test MCC: 51.64%\n"
     ]
    }
   ],
   "source": [
    "evaluate(args)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad783e95c017c6a0ffc7d9d3277599f38f5f101d4b63a62b683952e68d1e005f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
