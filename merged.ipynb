{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: 2021년 국립국어원 인공지능 언어능력 평가\n",
    "\n",
    "- [2021년 국립국어원 인공지능 언어능력 평가](https://corpus.korean.go.kr/task/taskList.do?taskId=1&clCd=END_TASK&subMenuId=sub01) 는 9월 1일부터 시작하여 11월 1일까지 마감된 [네 가지 과제에](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02) 대한 언어능력 평가 대회\n",
    "- 여기서 제시된 과제를 그대로 수행하여 그 결과를 [최종 선정된 결과들](https://corpus.korean.go.kr/task/taskLeaderBoard.do?taskId=4&clCd=END_TASK&subMenuId=sub04)과 비교할 수 있도록 수행\n",
    "- 아직 테스트 셋의 정답이 공식적으로 공개되고 있지 않아, 네 가지 과제의 자료에서 evaluation dataset으로 가지고 성능을 비교할 계획\n",
    "- 기말 발표전까지 정답셋이 공개될 경우 이 정답셋을 가지고 성능 검증\n",
    "- Transformers 기반 방법론, 신경망 등 각자 생각한 방법대로 구현 가능\n",
    "- 현재 대회기간이 종료되어 자료가 다운로드 가능하지 않으니 첨부된 자료 참조\n",
    "- 개인적으로 하거나 최대 두명까지 그룹 허용. \n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 FinalProject_[DS또는 CL]_학과_이름.ipynb\n",
    "- 마감 12월 6일(월) 23:59분까지.\n",
    "- 12월 7일, 9일 기말 발표 presentation 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리더보드\n",
    "\n",
    "- 최종발표전까지 각조는 각 태스크별 실행성능을 **시도된 여러 방법의 결과들을 지속적으로**  [리더보드](https://docs.google.com/spreadsheets/d/1-uenfp5GolpY2Gf0TsFbODvj585IIiFKp9fvYxcfgkY/edit#gid=0)에 해당 팀명(구성원 이름 포함)을 입력하여 공개하여야 함. \n",
    "- 최종 마감일에 이 순위와 실제 제출한 프로그램의 수행 결과를 비교하여 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYqodz4fUN51"
   },
   "source": [
    "# BoolQ (판정 의문문, 정현진)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIbd0ZBds4hT"
   },
   "source": [
    "### 기본 세팅 (colab pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638894220497,
     "user": {
      "displayName": "Jesper Jung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06572571306595621256"
     },
     "user_tz": -540
    },
    "id": "AbpFe5FCspCq",
    "outputId": "17538be5-3fb7-44fc-e65a-b9c2b6f418b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  7 16:23:40 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1638894223348,
     "user": {
      "displayName": "Jesper Jung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06572571306595621256"
     },
     "user_tz": -540
    },
    "id": "_Q11YOefs3gY",
    "outputId": "77149f9d-0479-4f68-a07e-fd630652f901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 27.3 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3428,
     "status": "ok",
     "timestamp": 1638894277390,
     "user": {
      "displayName": "Jesper Jung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06572571306595621256"
     },
     "user_tz": -540
    },
    "id": "0xxQkaZIop9_",
    "outputId": "0afd5b1a-b0e9-4e6c-ed8c-5dd5e57374cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARcVg_T5ET9Y"
   },
   "outputs": [],
   "source": [
    "cur_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu0hfU4fpLQB"
   },
   "source": [
    "### Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37349,
     "status": "ok",
     "timestamp": 1638891198730,
     "user": {
      "displayName": "­정현진 / 학생 / 지능정보융합학과",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939885582343227076"
     },
     "user_tz": -540
    },
    "id": "HjYaot57jdNQ",
    "outputId": "06fc5e5d-57b6-4d7e-8c33-a97fca50e011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 4.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 36.7 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 558 kB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 39.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 54.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 4.3 MB/s \n",
      "\u001b[?25hCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 50.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 49.7 MB/s \n",
      "\u001b[?25hCollecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Building wheels for collected packages: subprocess32, pathtools\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=1f6c64eafc878c4fc277437ba986c5a05294fd0b87d5916ba74e7a3a029de9ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=4375db4dfa80bf47c66ea1b31544d632b531be53cfe0b39eedd74d438d05b5cd\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built subprocess32 pathtools\n",
      "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
      "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.5.4-py3-none-any.whl (524 kB)\n",
      "\u001b[K     |████████████████████████████████| 524 kB 4.1 MB/s \n",
      "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 50.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n",
      "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
      "\u001b[K     |████████████████████████████████| 332 kB 28.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
      "Collecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 47.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 33.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 50.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.8)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 51.5 MB/s \n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 57.4 MB/s \n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9ebe5904578cf8ad276884888a8275f620dec34142a9dad5ee607d6a25891323\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built future\n",
      "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.4 torchmetrics-0.6.1 yarl-1.7.2\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!pip install pytorch-lightning\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFNdyYaLpPhS"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Coc0NRzFQ3aF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLOfhdZUpikO"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSpfbqTOp1l5"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "  \"\"\" Here type your configurations! \"\"\"\n",
    "  # paths\n",
    "  train_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Train.tsv\"\n",
    "  dev_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Dev.tsv\"\n",
    "  test_path = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/SKT_BoolQ_Test.tsv\"\n",
    "  train_dev_crop = False\n",
    "\n",
    "  # model\n",
    "  model_list = {\n",
    "      'roberta': \"klue/roberta-large\",\n",
    "      'bigbird': \"monologg/kobigbird-bert-base\",\n",
    "      'electra': 'monologg/koelectra-base-v3-discriminator'\n",
    "  }\n",
    "\n",
    "  num_classes = 2\n",
    "\n",
    "  # dataset\n",
    "  k_fold = 5\n",
    "  batch_size = 2\n",
    "\n",
    "  # optimizer, schedular\n",
    "  learning_rate = 8e-6\n",
    "  weight_decay = 0.01\n",
    "  warmup_steps = 500\n",
    "\n",
    "  # Save\n",
    "  log_interval = 10\n",
    "  mode_wandb = True\n",
    "  save_dir = \"/content/drive/MyDrive/Colab Notebooks/BoolQ_/result/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpVbp8Dqp3tk"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "priphVDhp5ZJ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BoolQ_Dataset(Dataset):\n",
    "  def __init__(self, config, training=True):\n",
    "    \"\"\" Configuration \"\"\" \n",
    "    self.config = config\n",
    "\n",
    "    if training: # for K folding\n",
    "      self.dataset = self.load_data(config.train_path)\n",
    "    else: # test data\n",
    "      self.dataset = self.load_data(config.dev_path)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ## Return text and label\n",
    "    return {\n",
    "        \"text\": self.dataset[\"text\"].values[idx], \n",
    "        \"question\": self.dataset[\"question\"].values[idx], \n",
    "        \"label\": self.dataset[\"label\"].values[idx]\n",
    "    }\n",
    "\n",
    "\n",
    "  def load_data(self, dataset_dir):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'text', 'question', 'answer'], header=0)\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int)\n",
    "    dataset['text'] = dataset['text'].apply(self.pre_process)\n",
    "    return dataset\n",
    "\n",
    "  def pre_process(self, st):\n",
    "    st = re.sub('\\(.*\\)|\\s-\\s.*', '', st)\n",
    "    st = re.sub('\\[.*\\]|\\s-\\s.*', '', st)\n",
    "    st = st.lower()\n",
    "\n",
    "    st = re.sub('[”“]', '\\\"', st)\n",
    "    st = re.sub('[’‘]', '\\'', st)\n",
    "    st = re.sub('[≫〉》＞』」]', '>', st)\n",
    "    st = re.sub('[《「『〈≪＜]','<',st)\n",
    "    st = re.sub('[−–—]', '−', st)\n",
    "    st = re.sub('[･•・‧]','·', st)\n",
    "    st = st.replace('／', '/')\n",
    "    st = st.replace('℃', '도')\n",
    "    st = st.replace('→', '에서')\n",
    "    st = st.replace('!', '')\n",
    "    st = st.replace('，', ',')\n",
    "    st = st.replace('㎢', 'km')\n",
    "    st = st.replace('∼', '~')\n",
    "    st = st.replace('㎜', 'mm')\n",
    "    st = st.replace('×', '곱하기')\n",
    "    st = st.replace('=', '는')\n",
    "    st = st.replace('®', '')\n",
    "    st = st.replace('㎖', 'ml')\n",
    "    st = st.replace('ℓ', 'l')\n",
    "    st = st.replace('˚C', '도')\n",
    "    st = st.replace('˚', '도')\n",
    "    st = st.replace('°C', '도')\n",
    "    st = st.replace('°', '도')\n",
    "    st = st.replace('＋', '+')\n",
    "    st = st.replace('*', '')\n",
    "    st = st.replace(';', '.')\n",
    "    return st\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1075,
     "status": "ok",
     "timestamp": 1638891209390,
     "user": {
      "displayName": "­정현진 / 학생 / 지능정보융합학과",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939885582343227076"
     },
     "user_tz": -540
    },
    "id": "7YEj7z71txLF",
    "outputId": "375a602b-330c-4cb8-86e7-888efccb4441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3665\n",
      "{'text': '로마 시대의 오리엔트의 범위는 제국 내에 동부 지방은 물론 제국 외부에 있는 다른 국가에 광범위하게 쓰이는 단어였다. 그 후에 로마 제국이 분열되고 서유럽이 그들의 중심적인 세계를 형성하는 과정에서 자신들을 옥시덴트, 서방이라 부르며 오리엔트는 이와 대조되는 문화를 가진 동방세계라는 뜻이 부가되어, 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어가 되었다.', 'question': '오리엔트는 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어로 쓰인다.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "test_data = BoolQ_Dataset(config)\n",
    "print(len(test_data))\n",
    "\n",
    "for data in test_data:\n",
    "  print(data)\n",
    "  break\n",
    "\n",
    "batch = test_data[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMs_RsrR4yxu"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZtHIWPU4ybn"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BigBirdModel,\n",
    "    BigBirdPreTrainedModel, \n",
    "    ElectraModel, \n",
    "    ElectraPreTrainedModel, \n",
    "    XLMRobertaModel, \n",
    "    BartModel, \n",
    "    BartPretrainedModel, \n",
    "    T5Model, \n",
    "    RobertaModel \n",
    ")\n",
    "\n",
    "\"\"\" KoBigBird Pre-trained Model \"\"\"\n",
    "\n",
    "class BigBird_BoolQ(BigBirdPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bigbird = BigBirdModel.from_pretrained(\n",
    "            \"monologg/kobigbird-bert-base\",\n",
    "            config=config\n",
    "        )  # Load pretrained bigbird\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate=0.1)\n",
    "        # l2 norm, similarity add\n",
    "        self.label_classifier = FCLayer(\n",
    "            config.hidden_size,\n",
    "            config.num_labels,\n",
    "            dropout_rate = 0.1, \n",
    "            use_activation=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bigbird(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\n",
    "\n",
    "        # Concat -> fc_layer\n",
    "        logits = self.label_classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" KoElectra Pre-trained Model \"\"\"\n",
    "\n",
    "class Electra_BoolQ(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Electra_BoolQ, self).__init__(config)\n",
    "\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = ElectraModel.from_pretrained(\n",
    "            'monologg/koelectra-base-v3-discriminator', config=config)\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1)\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0][:,0,:] #cls\n",
    "        sequence_output = self.pooling(sequence_output)\n",
    "        logits = self.qa_classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Roberta Pre-trained Model \"\"\"\n",
    "class Roberta_BoolQ(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(\"klue/roberta-large\", config=config)  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1)\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.roberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        # pooled_output_cat = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "        \n",
    "        logits = self.qa_classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Additional Layers \"\"\"\n",
    "\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        inner_dim: int,\n",
    "        pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYWAZVo2ag3"
   },
   "source": [
    "### Training Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5301TGQkqSdC"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# https://visionhong.tistory.com/30\n",
    "# Here is the code for pl.\n",
    "\n",
    "class BoolQ_Model_Train():\n",
    "  def __init__(self, config, model_name):\n",
    "    super().__init__()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.device = device\n",
    "    self.config = config\n",
    "\n",
    "    #####################\n",
    "    ### Configuration ###\n",
    "    #####################\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "\n",
    "    assert model_name in config.model_list.keys(), \"[Training] Please Give Correct Model Name which have been listed.\"\n",
    "    self.model_name = model_name\n",
    "\n",
    "    # load configuration of pretrained model\n",
    "    MODEL_CONFIG = AutoConfig.from_pretrained(config.model_list[model_name])\n",
    "    MODEL_CONFIG.num_labels = 2\n",
    "\n",
    "    if model_name == \"roberta\":\n",
    "      self.model = Roberta_BoolQ(MODEL_CONFIG)\n",
    "    elif model_name == \"bigbird\":\n",
    "      self.model = BigBird_BoolQ(MODEL_CONFIG)\n",
    "    elif model_name == \"electra\":\n",
    "      self.model = Electra_BoolQ(MODEL_CONFIG)\n",
    "\n",
    "    self.model.to(device)\n",
    "\n",
    "\n",
    "    \"\"\" Tokenizer \"\"\"\n",
    "\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(config.model_list[model_name])\n",
    "\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "\n",
    "    # train_dataset\n",
    "    self.train_dataset = BoolQ_Dataset(config)\n",
    "\n",
    "    # k_fold index\n",
    "    skf_iris = StratifiedKFold(n_splits=config.k_fold)\n",
    "    self.kfold = config.k_fold\n",
    "    self.KFold_index = list(skf_iris.split(\n",
    "        self.train_dataset.dataset['text'], self.train_dataset.dataset['label']))\n",
    "    \n",
    "    # batch_size\n",
    "    self.batch_size = config.batch_size\n",
    "\n",
    "\n",
    "    \"\"\" optimizer, scheduler (in fit() function), criterion \"\"\"\n",
    "\n",
    "    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    \"\"\" Training Saving \"\"\"\n",
    "\n",
    "    self.log_interval = config.log_interval\n",
    "    self.load_step = 0\n",
    "    self.best_acc = 0\n",
    "    self.wandb = config.mode_wandb\n",
    "    self.save_dir = config.save_dir\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self, epoch):\n",
    "    # schedular\n",
    "    self.scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "      self.optimizer, \n",
    "      num_warmup_steps=config.warmup_steps, \n",
    "      num_training_steps=len(self.train_dataset) * epoch, \n",
    "      last_epoch= -1\n",
    "    )\n",
    "\n",
    "    \n",
    "    \"\"\" GO TRAINING. \"\"\"\n",
    "    self.epoch = epoch\n",
    "\n",
    "    for epo in tqdm(range(epoch)):\n",
    "      ### Stratified KFold\n",
    "      train_idx, val_idx = self.KFold_index[epo % self.kfold]\n",
    "\n",
    "      training_set = Subset(self.train_dataset, train_idx)\n",
    "      validation_set = Subset(self.train_dataset, val_idx)\n",
    "\n",
    "      ### make dataloader\n",
    "      train_loader = DataLoader(training_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "      val_loader = DataLoader(validation_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "      ### train\n",
    "      self.training_step(train_loader, epo)\n",
    "\n",
    "      ### val\n",
    "      self.validation_step(val_loader, epo)\n",
    "\n",
    "      ### Best model save\n",
    "      if self.best_acc < self.val_acc:\n",
    "        self.best_acc = self.val_acc\n",
    "\n",
    "        print(\"Best Model Saving!\")\n",
    "\n",
    "        model_to_save = self.model.module if hasattr(model, \"module\") else self.model\n",
    "        model_to_save.save_pretrained(f\"{self.save_dir}/best/{self.model_name}\")\n",
    "        torch.save(self.config, os.path.join(f\"{save_dir}/best/{self.model_name}\", \"training_config.bin\"))\n",
    "\n",
    "\n",
    "      \n",
    "      \n",
    "\n",
    "  def training_step(self, train_loader, epo):\n",
    "    # allocate model to train mode\n",
    "    self.model.train()\n",
    "    tot_acc, tot_loss = 0., 0.\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "      pbar = tqdm(total = len(train_loader), desc=\"[Training] Epoch {}\".format(epo+1))\n",
    "      ### allocate to cuda or not.\n",
    "      # texts -> cpu tensor, labels -> array.\n",
    "      # texts: {input_ids, token_type_ids, attention_mask}\n",
    "      texts = {key: torch.tensor(value).to(self.device) for key, value in texts.items()}\n",
    "      labels = torch.tensor(labels).to(self.device)\n",
    "\n",
    "      ###########################################\n",
    "      # 1) zero_grad\n",
    "      self.optimizer.zero_grad()\n",
    "\n",
    "      # 2) forward\n",
    "      y_pred = self.model(**texts)[0]\n",
    "\n",
    "      # 3) calculate loss\n",
    "      loss = self.criterion(y_pred, labels)\n",
    "\n",
    "      # 4) backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5) optimier step\n",
    "      self.optimizer.step()\n",
    "\n",
    "      # 6) schedular step\n",
    "      self.schedular.step()\n",
    "\n",
    "      ###########################################\n",
    "\n",
    "\n",
    "      ### update, and cumulate match and loss\n",
    "      pbar.update()\n",
    "      self.load_step += 1\n",
    "\n",
    "      preds = torch.argmax(y_pred, dim=-1)\n",
    "      tot_loss += loss.item()\n",
    "      tot_acc += (preds == labels).sum().item() / self.batch_size\n",
    "\n",
    "      ### saving to log\n",
    "      if self.load_step % self.log_interval == 0:\n",
    "        train_loss = tot_loss / self.log_interval\n",
    "        train_acc = tot_acc / self.log_interval\n",
    "        current_lr = self.get_lr(self.optimizer)\n",
    "\n",
    "        pbar.set_description(f\"Epoch: [{epo}/{self.epochs}]({self.load_step}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n",
    "\n",
    "        self.train_loss = train_loss\n",
    "        self.train_acc = train_acc\n",
    "        self.current_lr = current_lr\n",
    "\n",
    "        tot_acc, tot_value = 0., 0.\n",
    "\n",
    "\n",
    "\n",
    "  def validation_step(self, val_loader, epo):\n",
    "    # allocate model to eval mode\n",
    "    self.model.eval()\n",
    "    tot_acc, tot_loss = 0., 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for texts, labels in val_loader:\n",
    "        pbar = tqdm(total = len(val_loader), desc=\"[Validation] Epoch {}\".format(epo+1))\n",
    "        ### allocate to cuda or not.\n",
    "        # texts -> cpu tensor, labels -> array.\n",
    "        # texts: {input_ids, token_type_ids, attention_mask}\n",
    "        texts = {key: torch.tensor(value).to(self.device) for key, value in texts.items()}\n",
    "        labels = torch.tensor(labels).to(self.device)\n",
    "\n",
    "        ###########################################\n",
    "        # 1) forward\n",
    "        y_pred = self.model(**texts)[0]\n",
    "\n",
    "        # 2) calculate loss\n",
    "        loss = self.criterion(y_pred, labels)\n",
    "\n",
    "        ###########################################\n",
    "        \"\"\" Update and save loss \"\"\"\n",
    "\n",
    "        pbar.update()\n",
    "    \n",
    "        preds = torch.argmax(y_pred, dim=-1)\n",
    "        tot_loss += loss.item()\n",
    "        tot_acc += (preds == labels).sum().item() / self.batch_size\n",
    "\n",
    "        ############################################\n",
    "        \n",
    "\n",
    "    val_loss = tot_loss / len(val_loader)\n",
    "    val_acc = tot_acc / len(val_loader)\n",
    "\n",
    "    pbar.set_description(f\"Validation: [{epo}/{self.epochs}] || loss: {val_loss:4.4} || acc: {val_acc:4.2%}\")\n",
    "\n",
    "    if self.wandb:\n",
    "        wandb.log({\"train_loss\": self.train_loss, \"train_acc\": self.train_acc,\n",
    "            \"lr\":self.current_lr, \"valid_loss\":val_loss, \"valid_acc\":val_acc\n",
    "        })\n",
    "\n",
    "    self.val_acc = val_acc\n",
    "\n",
    "\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "    \"\"\"\n",
    "      Collate a batch of dataset to same length of text.\n",
    "\n",
    "    ? INPUT\n",
    "    dataset: {text: string, question: string, label: int}\n",
    "\n",
    "    ? OUTPUT\n",
    "    padded token ids.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # integrate from dataset (dict) into list\n",
    "    text_list = [b['text'] for b in batch]\n",
    "    query_list = [b['question'] for b in batch]\n",
    "    label_list = [b['label'] for b in batch]\n",
    "    \n",
    "    # tokenize\n",
    "    text_query_list = list(zip(text_list, query_list))\n",
    "\n",
    "    if self.model_name == 'bigbird':\n",
    "      max_length = 1024\n",
    "    else:\n",
    "      max_length = 512\n",
    "\n",
    "    tokenized_sentence = self.tokenizer(\n",
    "        text_query_list,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    # output of tokenized_sentence: {input_ids, token_type_ids, attention_mask}\n",
    "    return tokenized_sentence, label_list\n",
    "\n",
    "  def get_lr(self, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "      return param_group['lr']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636,
     "referenced_widgets": [
      "13a2f4649e8047a8afeaf03f59d33279",
      "81d37d49671547e6b5cf5f9796a7c341",
      "c6ddc2206f25455a8fcabbdb4ca07b74",
      "5b074748a57e4cbab17fa772b48e58b4",
      "abe820ab8fb4420b90c0cad00f1bd870",
      "73c019de2e974d9d981fc14c7dad42f7",
      "e73ff60e42c54fa4a436d301f1e77159",
      "7bf7d1a4e2b04682b2f1a08eb8b2ecd5"
     ]
    },
    "executionInfo": {
     "elapsed": 12563,
     "status": "error",
     "timestamp": 1638891651283,
     "user": {
      "displayName": "­정현진 / 학생 / 지능정보융합학과",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939885582343227076"
     },
     "user_tz": -540
    },
    "id": "sNtOL89QdxQk",
    "outputId": "eb0dd1fd-9c99-4429-f220-56ae57fb2af0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29boo4zv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 518... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a2f4649e8047a8afeaf03f59d33279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hello</strong>: <a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/29boo4zv\" target=\"_blank\">https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/29boo4zv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211207_153726-29boo4zv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29boo4zv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ/runs/2nlkh9l7\" target=\"_blank\">hello</a></strong> to <a href=\"https://wandb.ai/jesper_jung/HyunJin-BoolQ\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8fe66a32a75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'HyunJin-BoolQ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoolQ_Model_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bigbird'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d55df1cdde9c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, model_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectra_BoolQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.34 GiB already allocated; 9.81 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if config.mode_wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(project='HyunJin-BoolQ', name=\"hello\")\n",
    "\n",
    "Trainer = BoolQ_Model_Train(config, 'bigbird')\n",
    "Trainer.fit(epoch = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTHtMZkcdrMc"
   },
   "source": [
    "##### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV7rvaaI9UDE"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BigBirdTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "dataset = BoolQ_Dataset(config)\n",
    "print(dataset)\n",
    "idx = np.asarray([1, 3, 5, 6])\n",
    "print(Subset(dataset, idx))\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 8,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "for batch, label_list in loader:\n",
    "  print(batch)\n",
    "  print(batch['input_ids'].shape)\n",
    "  print(batch['token_type_ids'].shape)\n",
    "  print(batch['attention_mask'].shape)\n",
    "\n",
    "  print(tokenizer.batch_decode(batch['input_ids'].tolist()))\n",
    "  break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e452ec34",
   "metadata": {},
   "source": [
    "# 여기서부터 COPA (ㅇ\n",
    "---\n",
    "- Pretrained Model : klue/Roberta-large "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e2504",
   "metadata": {},
   "source": [
    "- 아래 실행하여 라이브러리 설치\n",
    "-``` pip install BackTranslation```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27036e06",
   "metadata": {},
   "source": [
    "## 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae7df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from time import sleep\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    ElectraModel,\n",
    "    ElectraPreTrainedModel,\n",
    "    XLMRobertaModel,\n",
    "    BartModel,\n",
    "    BartPretrainedModel,\n",
    "    T5Model,\n",
    "    RobertaModel,\n",
    ")\n",
    "from transformers import MBartModel, MBartConfig\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from BackTranslation import BackTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b65e38",
   "metadata": {},
   "source": [
    "## Data Augmentation by Backtranslation\n",
    "---\n",
    "- Google Translation 사용\n",
    "- 매우 오래걸리므로 전처리된 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276e2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data = \"dataset/copa/SKT_COPA_Train.tsv\"\n",
    "augmented_train_data = \"dataset/copa/SKT_COPA_Train_aug.tsv\"\n",
    "valid_data = \"dataset/copa/SKT_COPA_Dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0641ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    original_train_data,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"ID\", \"sentence\", \"question\", \"1\", \"2\", \"answer\"],\n",
    "    header=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2945f5",
   "metadata": {},
   "source": [
    "- 아래와 같은 코드를 사용하여 backtranslate하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445b4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_backtranslated = \"dataset/copa/en_new_sentences.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782e0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans = BackTranslation(url=['translate.google.co.kr',])\n",
    "# def augment_sentence(trans, s, tmp='en'):\n",
    "#     return trans.translate(s, src='ko', tmp=tmp).result_text\n",
    "# tmps = [en']\n",
    "# new_datasets = dict()\n",
    "# new_sentences = dict()\n",
    "\n",
    "# for tmp in tmps:\n",
    "#     new_dataset = copy.deepcopy(dataset)\n",
    "#     sentences = new_dataset['sentence'].tolist()\n",
    "#     new_sentences[tmp] = list()\n",
    "#     for sent in tqdm(sentences):\n",
    "#         new_sentences[tmp].append(augment_sentence(trans, sent, tmp=tmp))\n",
    "\n",
    "# torch.save(new_sentences, saved_backtranslated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b2c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_en = torch.load(saved_backtranslated)\n",
    "sent = dict()\n",
    "sent['en'] = sent_en['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701d83c",
   "metadata": {},
   "source": [
    "## 원본 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a453fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>question</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>이퀄라이저로 저음 음역대 소리 크기를 키웠다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>베이스 소리가 잘 들리게 되었다.</td>\n",
       "      <td>베이스 소리가 들리지 않게 되었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>음료에 초콜렛 시럽을 넣었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>음료수가 더 달아졌다.</td>\n",
       "      <td>음료수가 차가워졌다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>남자는 휴대폰을 호수에 빠뜨렸다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>휴대폰이 업그레이드 되었다.</td>\n",
       "      <td>휴대폰이 고장났다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>옆 집 사람이 이사를 나갔다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>옆 집 사람은 계약이 완료되었다.</td>\n",
       "      <td>옆 집 사람은 계약을 연장했다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>문을 밀었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>문이 잠겼다.</td>\n",
       "      <td>문이 열렸다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                   sentence question                   1  \\\n",
       "0   1  이퀄라이저로 저음 음역대 소리 크기를 키웠다.       결과  베이스 소리가 잘 들리게 되었다.   \n",
       "1   2           음료에 초콜렛 시럽을 넣었다.       결과        음료수가 더 달아졌다.   \n",
       "2   3         남자는 휴대폰을 호수에 빠뜨렸다.       결과     휴대폰이 업그레이드 되었다.   \n",
       "3   4           옆 집 사람이 이사를 나갔다.       원인  옆 집 사람은 계약이 완료되었다.   \n",
       "4   5                    문을 밀었다.       결과             문이 잠겼다.   \n",
       "\n",
       "                     2  answer  \n",
       "0  베이스 소리가 들리지 않게 되었다.       1  \n",
       "1          음료수가 차가워졌다.       1  \n",
       "2           휴대폰이 고장났다.       2  \n",
       "3    옆 집 사람은 계약을 연장했다.       1  \n",
       "4              문이 열렸다.       2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5769d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = copy.deepcopy(dataset)\n",
    "new_dataset['ID'] += len(dataset)\n",
    "new_dataset['sentence'] = sent['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd89c11",
   "metadata": {},
   "source": [
    "## BackTranslate로 augment한 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0958d811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>question</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3081</td>\n",
       "      <td>이퀄라이저는베이스 스캔의 사운드를 올렸습니다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>베이스 소리가 잘 들리게 되었다.</td>\n",
       "      <td>베이스 소리가 들리지 않게 되었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3082</td>\n",
       "      <td>나는 음료에 초콜릿 시럽을 넣었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>음료수가 더 달아졌다.</td>\n",
       "      <td>음료수가 차가워졌다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3083</td>\n",
       "      <td>그 남자는 호수에 휴대 전화를 넣었습니다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>휴대폰이 업그레이드 되었다.</td>\n",
       "      <td>휴대폰이 고장났다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3084</td>\n",
       "      <td>집 옆에있는 사람이 나갔다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>옆 집 사람은 계약이 완료되었다.</td>\n",
       "      <td>옆 집 사람은 계약을 연장했다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3085</td>\n",
       "      <td>나는 문을 밀었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>문이 잠겼다.</td>\n",
       "      <td>문이 열렸다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                   sentence question                   1  \\\n",
       "0  3081  이퀄라이저는베이스 스캔의 사운드를 올렸습니다.       결과  베이스 소리가 잘 들리게 되었다.   \n",
       "1  3082        나는 음료에 초콜릿 시럽을 넣었다.       결과        음료수가 더 달아졌다.   \n",
       "2  3083    그 남자는 호수에 휴대 전화를 넣었습니다.       결과     휴대폰이 업그레이드 되었다.   \n",
       "3  3084            집 옆에있는 사람이 나갔다.       원인  옆 집 사람은 계약이 완료되었다.   \n",
       "4  3085                 나는 문을 밀었다.       결과             문이 잠겼다.   \n",
       "\n",
       "                     2  answer  \n",
       "0  베이스 소리가 들리지 않게 되었다.       1  \n",
       "1          음료수가 차가워졌다.       1  \n",
       "2           휴대폰이 고장났다.       2  \n",
       "3    옆 집 사람은 계약을 연장했다.       1  \n",
       "4              문이 열렸다.       2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa91a90",
   "metadata": {},
   "source": [
    "## 데이터 합병"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "885eb2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>question</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>이퀄라이저로 저음 음역대 소리 크기를 키웠다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>베이스 소리가 잘 들리게 되었다.</td>\n",
       "      <td>베이스 소리가 들리지 않게 되었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>음료에 초콜렛 시럽을 넣었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>음료수가 더 달아졌다.</td>\n",
       "      <td>음료수가 차가워졌다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>남자는 휴대폰을 호수에 빠뜨렸다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>휴대폰이 업그레이드 되었다.</td>\n",
       "      <td>휴대폰이 고장났다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>옆 집 사람이 이사를 나갔다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>옆 집 사람은 계약이 완료되었다.</td>\n",
       "      <td>옆 집 사람은 계약을 연장했다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>문을 밀었다.</td>\n",
       "      <td>결과</td>\n",
       "      <td>문이 잠겼다.</td>\n",
       "      <td>문이 열렸다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>6156</td>\n",
       "      <td>계약자로 일한 남자들은 떠났다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>계약을 연장했다.</td>\n",
       "      <td>계약이 종료되었다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>6157</td>\n",
       "      <td>목 마른.</td>\n",
       "      <td>원인</td>\n",
       "      <td>물을 마시지 못했다.</td>\n",
       "      <td>텀블러를 샀다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>6158</td>\n",
       "      <td>나는 그 노래를 오랫동안 전화 했어.</td>\n",
       "      <td>결과</td>\n",
       "      <td>목이 아프다.</td>\n",
       "      <td>노래방이 폐업했다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>6159</td>\n",
       "      <td>사람들은 한 번 함께 일하고 있습니다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>우리나라 축구팀이 골을 넣었다.</td>\n",
       "      <td>우리나라 축구팀이 경기에서 패배했다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>6160</td>\n",
       "      <td>가수의 목이 쉬워졌습니다.</td>\n",
       "      <td>원인</td>\n",
       "      <td>가수가 3시간 동안 춤을 추었다.</td>\n",
       "      <td>가수가 3시간 동안 노래를 불렀다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6160 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                   sentence question                   1  \\\n",
       "0        1  이퀄라이저로 저음 음역대 소리 크기를 키웠다.       결과  베이스 소리가 잘 들리게 되었다.   \n",
       "1        2           음료에 초콜렛 시럽을 넣었다.       결과        음료수가 더 달아졌다.   \n",
       "2        3         남자는 휴대폰을 호수에 빠뜨렸다.       결과     휴대폰이 업그레이드 되었다.   \n",
       "3        4           옆 집 사람이 이사를 나갔다.       원인  옆 집 사람은 계약이 완료되었다.   \n",
       "4        5                    문을 밀었다.       결과             문이 잠겼다.   \n",
       "...    ...                        ...      ...                 ...   \n",
       "3075  6156          계약자로 일한 남자들은 떠났다.       원인           계약을 연장했다.   \n",
       "3076  6157                      목 마른.       원인         물을 마시지 못했다.   \n",
       "3077  6158       나는 그 노래를 오랫동안 전화 했어.       결과             목이 아프다.   \n",
       "3078  6159      사람들은 한 번 함께 일하고 있습니다.       원인   우리나라 축구팀이 골을 넣었다.   \n",
       "3079  6160             가수의 목이 쉬워졌습니다.       원인  가수가 3시간 동안 춤을 추었다.   \n",
       "\n",
       "                         2  answer  \n",
       "0      베이스 소리가 들리지 않게 되었다.       1  \n",
       "1              음료수가 차가워졌다.       1  \n",
       "2               휴대폰이 고장났다.       2  \n",
       "3        옆 집 사람은 계약을 연장했다.       1  \n",
       "4                  문이 열렸다.       2  \n",
       "...                    ...     ...  \n",
       "3075            계약이 종료되었다.       2  \n",
       "3076              텀블러를 샀다.       1  \n",
       "3077            노래방이 폐업했다.       1  \n",
       "3078  우리나라 축구팀이 경기에서 패배했다.       2  \n",
       "3079   가수가 3시간 동안 노래를 불렀다.       2  \n",
       "\n",
       "[6160 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = dataset.append(new_dataset)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694f3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv(augmented_train_data, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df718ea",
   "metadata": {},
   "source": [
    "# COPA 학습 & Inference to json 코드\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3856b",
   "metadata": {},
   "source": [
    "## Transformers의 Wrapper Class와 일부 테스트 모델 선언 및 구현부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcef7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, inner_dim: int, pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Bert(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Bert, self).__init__(config)\n",
    "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class XLMRoberta(XLMRobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(XLMRoberta, self).__init__(config)\n",
    "        self.xlmroberta = XLMRobertaModel.from_pretrained(\n",
    "            \"xlm-roberta-large\", config=config\n",
    "        )  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.xlmroberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.xlmroberta(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class Electra_BoolQ(ElectraPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Electra_BoolQ, self).__init__(config)\n",
    "\n",
    "        # self.num_labels = config.num_labels\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = ElectraModel.from_pretrained(\n",
    "            \"monologg/koelectra-base-v3-discriminator\", config=config\n",
    "        )\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "        # self.sparse = Sparsemax()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.model(\n",
    "            input_ids2, attention_mask=attention_mask2, token_type_ids=token_type_ids2\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class Roberta(RobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            \"klue/roberta-large\", config=config\n",
    "        )  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.roberta(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a14d2",
   "metadata": {},
   "source": [
    "## 데이터 전처리부\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5845e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def load_data(dataset_dir):\n",
    "    dataset = pd.read_csv(\n",
    "        dataset_dir,\n",
    "        delimiter=\"\\t\",\n",
    "        names=[\"ID\", \"sentence\", \"question\", \"1\", \"2\", \"answer\"],\n",
    "        header=0,\n",
    "    )\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int) - 1\n",
    "\n",
    "    new_sentence1_1 = []\n",
    "    new_sentence1_2 = []\n",
    "    new_sentence2_1 = []\n",
    "    new_sentence2_2 = []\n",
    "    for i in range(len(dataset)):\n",
    "        s = dataset.iloc[i][\"sentence\"]\n",
    "        q = dataset.iloc[i][\"question\"]\n",
    "        s1 = dataset.iloc[i][\"1\"]\n",
    "        s2 = dataset.iloc[i][\"2\"]\n",
    "        lb = dataset.iloc[i][\"label\"]\n",
    "        if q == \"결과\":\n",
    "            new_sentence1_1.append(\"[결과]\" + s)\n",
    "            # new_sentence1_1.append(s)\n",
    "            new_sentence1_2.append(s1)\n",
    "            new_sentence2_1.append(\"[결과]\" + s)\n",
    "            # new_sentence2_1.append(s)\n",
    "            new_sentence2_2.append(s2)\n",
    "\n",
    "        else:\n",
    "            new_sentence1_1.append(\"[원인]\" + s1)\n",
    "            # new_sentence1_1.append(s1)\n",
    "            new_sentence1_2.append(s)\n",
    "            new_sentence2_1.append(\"[원인]\" + s2)\n",
    "            # new_sentence2_1.append(s2)\n",
    "            new_sentence2_2.append(s)\n",
    "\n",
    "    dataset[\"new_sentence1_1\"] = new_sentence1_1\n",
    "    dataset[\"new_sentence1_2\"] = new_sentence1_2\n",
    "    dataset[\"new_sentence2_1\"] = new_sentence2_1\n",
    "    dataset[\"new_sentence2_2\"] = new_sentence2_2\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence1_1 = dataset[\"new_sentence1_1\"].tolist()\n",
    "    sentence1_2 = dataset[\"new_sentence1_2\"].tolist()\n",
    "    sentence2_1 = dataset[\"new_sentence2_1\"].tolist()\n",
    "    sentence2_2 = dataset[\"new_sentence2_2\"].tolist()\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentence1_1,\n",
    "        sentence1_2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    tokenized_sentences2 = tokenizer(\n",
    "        sentence2_1,\n",
    "        sentence2_2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    for key, value in tokenized_sentences2.items():\n",
    "        tokenized_sentences[key + \"2\"] = value\n",
    "\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046a4c9",
   "metadata": {},
   "source": [
    "## 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccaa0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "    archs = {\n",
    "        \"encoder\": [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "        \"encoder-decoder\": [\"T5\", \"Bart\", \"Bart_BoolQ\"],\n",
    "    }\n",
    "    for arch in archs:\n",
    "        if model_type in archs[arch]:\n",
    "            return arch\n",
    "    raise ValueError(f\"Model [{model_type}] no defined archtecture\")\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }\n",
    "\n",
    "def increment_output_dir(output_path, exist_ok=False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\"\n",
    "\n",
    "def train(model_dir, args):\n",
    "\n",
    "    seed_everything(args.seed)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device(GPU) : {torch.cuda.is_available()}\")\n",
    "    num_classes = 2\n",
    "\n",
    "    # load model and tokenizerƒ\n",
    "    MODEL_NAME = args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_data(augmented_train_data)\n",
    "    val_dataset = load_data(valid_data)\n",
    "\n",
    "    train_label = train_dataset[\"label\"].values\n",
    "    val_label = val_dataset[\"label\"].values\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(\n",
    "        train_dataset, tokenizer, check_arch(args.model_type)\n",
    "    )\n",
    "    tokenized_val = tokenized_dataset(\n",
    "        val_dataset, tokenizer, check_arch(args.model_type)\n",
    "    )\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    train_dataset = CustomDataset(tokenized_train, train_label)\n",
    "    val_dataset = CustomDataset(tokenized_val, val_label)\n",
    "    # -- data_loader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=args.valid_batch_size, shuffle=False, drop_last=False,\n",
    "    )\n",
    "\n",
    "    # setting model hyperparameter\n",
    "    if args.model_type == \"Electra_BoolQ\":\n",
    "        config_module = ElectraConfig\n",
    "    else:\n",
    "        config_module = getattr(\n",
    "            import_module(\"transformers\"), args.model_type + \"Config\"\n",
    "        )\n",
    "\n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model_module = eval(args.model_type)\n",
    "\n",
    "    if args.model_type in [\"BERT\", \"Electra\"]:\n",
    "        model = model_module.from_pretrained(\n",
    "            MODEL_NAME, config=model_config, args=args\n",
    "        )\n",
    "    else:\n",
    "        model = model_module(config=model_config, args=args)\n",
    "\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    save_dir = increment_output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "    # Freeze Parameter\n",
    "    for name, param in model.named_parameters():\n",
    "        if (\"cls_fc_layer\" not in name) and (\n",
    "            \"label_classifier\" not in name\n",
    "        ):  # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay, eps=1e-8\n",
    "    )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=len(train_loader) * args.epochs,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "\n",
    "    # -- logging\n",
    "    start_time = time.time()\n",
    "    logger = SummaryWriter(log_dir=save_dir)\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vars(args), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        # train loop\n",
    "        # unFreeze parameters\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(train_loader):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item[\"labels\"])\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item[\"labels\"]).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                logger.add_scalar(\n",
    "                    \"Train/loss\", train_loss, epoch * len(train_loader) + idx\n",
    "                )\n",
    "                logger.add_scalar(\n",
    "                    \"Train/accuracy\", train_acc, epoch * len(train_loader) + idx\n",
    "                )\n",
    "                logger.add_scalar(\"LR\", current_lr, epoch * len(train_loader) + idx)\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            acc_okay = 0\n",
    "            count_all = 0\n",
    "            for idx, items in enumerate(tqdm(val_loader)):\n",
    "                sleep(0.01)\n",
    "                item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "                outs = model(**item)\n",
    "\n",
    "                preds = torch.argmax(outs[0], dim=-1)\n",
    "                loss = criterion(outs[0], item[\"labels\"]).item()\n",
    "\n",
    "                acc_item = (item[\"labels\"] == preds).sum().item()\n",
    "\n",
    "                val_loss_items.append(loss)\n",
    "                val_acc_items.append(acc_item)\n",
    "                acc_okay += acc_item\n",
    "                count_all += len(preds)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "            val_acc = acc_okay / count_all\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\n",
    "                    f\"New best model for val acc : {val_acc:4.2%}! saving the best model..\"\n",
    "                )\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.4}\"\n",
    "            )\n",
    "\n",
    "            logger.add_scalar(\"Val/loss\", val_loss, epoch)\n",
    "            logger.add_scalar(\"Val/accuracy\", val_acc, epoch)\n",
    "            s = f\"Time elapsed: {(time.time() - start_time)/60: .2f} min\"\n",
    "            print(s)\n",
    "            print()\n",
    "            if epoch > 24:\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb487ed6",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "---\n",
    "1. Roberta-large pretrained model 사용하여 fine-tune\n",
    "2. 10epoch 내외로 수렴하는 것을 확인해서 15epoch만 돌림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "args  = EasyDict(dict(\n",
    "    epochs = 15,\n",
    "    model_type = \"Roberta\",\n",
    "    pretrained_model = \"klue/roberta-large\",\n",
    "    lr = 8e-6,\n",
    "    batch_size = 32,\n",
    "    freeze_epoch = 0,\n",
    "    valid_batch_size = 128,\n",
    "    val_ratio = 0.2,\n",
    "    dropout_rate = 0.1,\n",
    "    criterion = 'cross_entropy',\n",
    "    optimizer = 'AdamW',\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = 500,\n",
    "    seed = 42,\n",
    "    log_interval = 20,\n",
    "    kfold = 1,\n",
    "    model_dir = \"./copa_data_results/results\",\n",
    "))\n",
    "    \n",
    "    \n",
    "    \n",
    "args.name = f'TrainAll_{args.model_type}_{args.lr}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3b9cc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cccef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(GPU) : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/15](20/192) || training loss 0.6938 || training accuracy 50.62% || lr 3.2e-07\n",
      "Epoch[0/15](40/192) || training loss 0.6936 || training accuracy 48.44% || lr 6.4e-07\n",
      "Epoch[0/15](60/192) || training loss 0.6956 || training accuracy 47.34% || lr 9.6e-07\n",
      "Epoch[0/15](80/192) || training loss 0.6906 || training accuracy 52.81% || lr 1.28e-06\n",
      "Epoch[0/15](100/192) || training loss 0.6913 || training accuracy 54.22% || lr 1.6e-06\n",
      "Epoch[0/15](120/192) || training loss 0.6918 || training accuracy 53.59% || lr 1.92e-06\n",
      "Epoch[0/15](140/192) || training loss 0.6883 || training accuracy 56.41% || lr 2.24e-06\n",
      "Epoch[0/15](160/192) || training loss 0.6875 || training accuracy 54.53% || lr 2.56e-06\n",
      "Epoch[0/15](180/192) || training loss 0.6704 || training accuracy 61.09% || lr 2.88e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5386d635dcc4d9aa8b4705ed887c2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 78.80%! saving the best model..\n",
      "[Val] acc : 78.80%, loss: 0.4907|| best acc : 78.80%, best loss: 0.4907\n",
      "Time elapsed:  1.08 min\n",
      "\n",
      "Epoch[1/15](20/192) || training loss 0.4782 || training accuracy 77.50% || lr 3.392e-06\n",
      "Epoch[1/15](40/192) || training loss 0.381 || training accuracy 84.06% || lr 3.712e-06\n",
      "Epoch[1/15](60/192) || training loss 0.4154 || training accuracy 82.19% || lr 4.032e-06\n",
      "Epoch[1/15](80/192) || training loss 0.3368 || training accuracy 85.62% || lr 4.352e-06\n",
      "Epoch[1/15](100/192) || training loss 0.3439 || training accuracy 85.47% || lr 4.6719999999999995e-06\n",
      "Epoch[1/15](120/192) || training loss 0.3751 || training accuracy 82.97% || lr 4.992e-06\n",
      "Epoch[1/15](140/192) || training loss 0.2885 || training accuracy 88.28% || lr 5.312e-06\n",
      "Epoch[1/15](160/192) || training loss 0.2783 || training accuracy 86.72% || lr 5.632e-06\n",
      "Epoch[1/15](180/192) || training loss 0.3064 || training accuracy 87.81% || lr 5.952e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feb5b838e38464dbf00014380d5eec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 91.20%! saving the best model..\n",
      "[Val] acc : 91.20%, loss: 0.2792|| best acc : 91.20%, best loss: 0.2792\n",
      "Time elapsed:  2.26 min\n",
      "\n",
      "Epoch[2/15](20/192) || training loss 0.1712 || training accuracy 93.28% || lr 6.464e-06\n",
      "Epoch[2/15](40/192) || training loss 0.1538 || training accuracy 92.97% || lr 6.784e-06\n",
      "Epoch[2/15](60/192) || training loss 0.1435 || training accuracy 94.38% || lr 7.104e-06\n",
      "Epoch[2/15](80/192) || training loss 0.1839 || training accuracy 92.50% || lr 7.424e-06\n",
      "Epoch[2/15](100/192) || training loss 0.1547 || training accuracy 94.38% || lr 7.743999999999999e-06\n",
      "Epoch[2/15](120/192) || training loss 0.1975 || training accuracy 92.81% || lr 7.986554621848738e-06\n",
      "Epoch[2/15](140/192) || training loss 0.1413 || training accuracy 95.00% || lr 7.919327731092437e-06\n",
      "Epoch[2/15](160/192) || training loss 0.141 || training accuracy 95.16% || lr 7.852100840336134e-06\n",
      "Epoch[2/15](180/192) || training loss 0.1112 || training accuracy 95.47% || lr 7.784873949579831e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0195235b67420d91d981cf78fa599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 91.60%! saving the best model..\n",
      "[Val] acc : 91.60%, loss: 0.3272|| best acc : 91.60%, best loss: 0.2792\n",
      "Time elapsed:  3.43 min\n",
      "\n",
      "Epoch[3/15](20/192) || training loss 0.06428 || training accuracy 97.34% || lr 7.677310924369748e-06\n",
      "Epoch[3/15](40/192) || training loss 0.066 || training accuracy 97.97% || lr 7.610084033613444e-06\n",
      "Epoch[3/15](60/192) || training loss 0.06924 || training accuracy 97.97% || lr 7.542857142857142e-06\n",
      "Epoch[3/15](80/192) || training loss 0.05448 || training accuracy 98.28% || lr 7.47563025210084e-06\n",
      "Epoch[3/15](100/192) || training loss 0.05794 || training accuracy 97.97% || lr 7.408403361344538e-06\n",
      "Epoch[3/15](120/192) || training loss 0.0687 || training accuracy 97.66% || lr 7.341176470588234e-06\n",
      "Epoch[3/15](140/192) || training loss 0.04584 || training accuracy 99.06% || lr 7.273949579831932e-06\n",
      "Epoch[3/15](160/192) || training loss 0.03607 || training accuracy 99.06% || lr 7.20672268907563e-06\n",
      "Epoch[3/15](180/192) || training loss 0.05669 || training accuracy 98.12% || lr 7.139495798319327e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e04832952a74a1fadcd92098cffc955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.60%, loss: 0.4094|| best acc : 91.60%, best loss: 0.2792\n",
      "Time elapsed:  4.46 min\n",
      "\n",
      "Epoch[4/15](20/192) || training loss 0.04117 || training accuracy 98.59% || lr 7.031932773109243e-06\n",
      "Epoch[4/15](40/192) || training loss 0.03569 || training accuracy 98.75% || lr 6.964705882352941e-06\n",
      "Epoch[4/15](60/192) || training loss 0.02685 || training accuracy 99.06% || lr 6.897478991596638e-06\n",
      "Epoch[4/15](80/192) || training loss 0.03934 || training accuracy 98.12% || lr 6.830252100840335e-06\n",
      "Epoch[4/15](100/192) || training loss 0.02512 || training accuracy 99.22% || lr 6.763025210084033e-06\n",
      "Epoch[4/15](120/192) || training loss 0.02224 || training accuracy 99.38% || lr 6.695798319327731e-06\n",
      "Epoch[4/15](140/192) || training loss 0.02148 || training accuracy 99.69% || lr 6.628571428571428e-06\n",
      "Epoch[4/15](160/192) || training loss 0.02032 || training accuracy 99.06% || lr 6.5613445378151255e-06\n",
      "Epoch[4/15](180/192) || training loss 0.02397 || training accuracy 99.38% || lr 6.4941176470588234e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937eb1dd6b4e41e28d12ce99fc6679f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 92.00%! saving the best model..\n",
      "[Val] acc : 92.00%, loss: 0.3837|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  5.64 min\n",
      "\n",
      "Epoch[5/15](20/192) || training loss 0.02074 || training accuracy 99.06% || lr 6.386554621848739e-06\n",
      "Epoch[5/15](40/192) || training loss 0.01713 || training accuracy 99.53% || lr 6.319327731092436e-06\n",
      "Epoch[5/15](60/192) || training loss 0.02199 || training accuracy 99.22% || lr 6.252100840336134e-06\n",
      "Epoch[5/15](80/192) || training loss 0.02969 || training accuracy 98.75% || lr 6.184873949579832e-06\n",
      "Epoch[5/15](100/192) || training loss 0.01875 || training accuracy 99.69% || lr 6.1176470588235285e-06\n",
      "Epoch[5/15](120/192) || training loss 0.0256 || training accuracy 98.91% || lr 6.0504201680672265e-06\n",
      "Epoch[5/15](140/192) || training loss 0.02173 || training accuracy 99.69% || lr 5.9831932773109244e-06\n",
      "Epoch[5/15](160/192) || training loss 0.01657 || training accuracy 99.53% || lr 5.9159663865546215e-06\n",
      "Epoch[5/15](180/192) || training loss 0.022 || training accuracy 99.22% || lr 5.848739495798319e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd4f1a66e744db19fbe0ab74ba77703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.20%, loss: 0.4203|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  6.66 min\n",
      "\n",
      "Epoch[6/15](20/192) || training loss 0.01329 || training accuracy 99.69% || lr 5.741176470588235e-06\n",
      "Epoch[6/15](40/192) || training loss 0.02177 || training accuracy 99.69% || lr 5.6739495798319324e-06\n",
      "Epoch[6/15](60/192) || training loss 0.01304 || training accuracy 99.69% || lr 5.6067226890756295e-06\n",
      "Epoch[6/15](80/192) || training loss 0.009519 || training accuracy 99.84% || lr 5.5394957983193275e-06\n",
      "Epoch[6/15](100/192) || training loss 0.01774 || training accuracy 99.53% || lr 5.472268907563025e-06\n",
      "Epoch[6/15](120/192) || training loss 0.01284 || training accuracy 99.69% || lr 5.4050420168067225e-06\n",
      "Epoch[6/15](140/192) || training loss 0.008861 || training accuracy 99.69% || lr 5.33781512605042e-06\n",
      "Epoch[6/15](160/192) || training loss 0.009139 || training accuracy 99.84% || lr 5.2705882352941176e-06\n",
      "Epoch[6/15](180/192) || training loss 0.01358 || training accuracy 99.53% || lr 5.203361344537815e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe25dfb07bf49de8fc8e5d4ecc9e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.80%, loss: 0.4458|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  7.69 min\n",
      "\n",
      "Epoch[7/15](20/192) || training loss 0.01839 || training accuracy 99.22% || lr 5.0957983193277305e-06\n",
      "Epoch[7/15](40/192) || training loss 0.01006 || training accuracy 99.69% || lr 5.0285714285714285e-06\n",
      "Epoch[7/15](60/192) || training loss 0.009921 || training accuracy 99.84% || lr 4.9613445378151256e-06\n",
      "Epoch[7/15](80/192) || training loss 0.006946 || training accuracy 99.84% || lr 4.8941176470588235e-06\n",
      "Epoch[7/15](100/192) || training loss 0.01625 || training accuracy 99.53% || lr 4.826890756302521e-06\n",
      "Epoch[7/15](120/192) || training loss 0.009226 || training accuracy 99.69% || lr 4.7596638655462185e-06\n",
      "Epoch[7/15](140/192) || training loss 0.01174 || training accuracy 99.69% || lr 4.692436974789916e-06\n",
      "Epoch[7/15](160/192) || training loss 0.005835 || training accuracy 100.00% || lr 4.625210084033614e-06\n",
      "Epoch[7/15](180/192) || training loss 0.007438 || training accuracy 99.84% || lr 4.557983193277311e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61da387e7a4448bb10ca7f32f39450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.80%, loss: 0.4472|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  8.72 min\n",
      "\n",
      "Epoch[8/15](20/192) || training loss 0.006374 || training accuracy 100.00% || lr 4.4504201680672266e-06\n",
      "Epoch[8/15](40/192) || training loss 0.006707 || training accuracy 99.84% || lr 4.3831932773109245e-06\n",
      "Epoch[8/15](60/192) || training loss 0.006601 || training accuracy 99.84% || lr 4.315966386554622e-06\n",
      "Epoch[8/15](80/192) || training loss 0.007672 || training accuracy 99.84% || lr 4.2487394957983195e-06\n",
      "Epoch[8/15](100/192) || training loss 0.005372 || training accuracy 99.84% || lr 4.181512605042017e-06\n",
      "Epoch[8/15](120/192) || training loss 0.008952 || training accuracy 99.69% || lr 4.114285714285714e-06\n",
      "Epoch[8/15](140/192) || training loss 0.006693 || training accuracy 99.69% || lr 4.047058823529412e-06\n",
      "Epoch[8/15](160/192) || training loss 0.004281 || training accuracy 100.00% || lr 3.979831932773109e-06\n",
      "Epoch[8/15](180/192) || training loss 0.003682 || training accuracy 100.00% || lr 3.912605042016807e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ab561033e4b8eb6259513978e0826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 92.00%, loss: 0.483|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  9.74 min\n",
      "\n",
      "Epoch[9/15](20/192) || training loss 0.002338 || training accuracy 100.00% || lr 3.805042016806722e-06\n",
      "Epoch[9/15](40/192) || training loss 0.003144 || training accuracy 100.00% || lr 3.73781512605042e-06\n",
      "Epoch[9/15](60/192) || training loss 0.001843 || training accuracy 100.00% || lr 3.670588235294117e-06\n",
      "Epoch[9/15](80/192) || training loss 0.009822 || training accuracy 99.53% || lr 3.603361344537815e-06\n",
      "Epoch[9/15](100/192) || training loss 0.007631 || training accuracy 99.69% || lr 3.5361344537815122e-06\n",
      "Epoch[9/15](120/192) || training loss 0.00453 || training accuracy 100.00% || lr 3.46890756302521e-06\n",
      "Epoch[9/15](140/192) || training loss 0.002018 || training accuracy 100.00% || lr 3.4016806722689073e-06\n",
      "Epoch[9/15](160/192) || training loss 0.005038 || training accuracy 100.00% || lr 3.3344537815126052e-06\n",
      "Epoch[9/15](180/192) || training loss 0.01228 || training accuracy 99.69% || lr 3.2672268907563023e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20631899fd4d433796ae354fd75fa3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.40%, loss: 0.4537|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  10.77 min\n",
      "\n",
      "Epoch[10/15](20/192) || training loss 0.001722 || training accuracy 100.00% || lr 3.159663865546218e-06\n",
      "Epoch[10/15](40/192) || training loss 0.004434 || training accuracy 100.00% || lr 3.092436974789916e-06\n",
      "Epoch[10/15](60/192) || training loss 0.004272 || training accuracy 99.84% || lr 3.0252100840336132e-06\n",
      "Epoch[10/15](80/192) || training loss 0.002358 || training accuracy 100.00% || lr 2.9579831932773108e-06\n",
      "Epoch[10/15](100/192) || training loss 0.001988 || training accuracy 100.00% || lr 2.8907563025210083e-06\n",
      "Epoch[10/15](120/192) || training loss 0.005524 || training accuracy 99.84% || lr 2.823529411764706e-06\n",
      "Epoch[10/15](140/192) || training loss 0.001401 || training accuracy 100.00% || lr 2.7563025210084033e-06\n",
      "Epoch[10/15](160/192) || training loss 0.005311 || training accuracy 99.84% || lr 2.689075630252101e-06\n",
      "Epoch[10/15](180/192) || training loss 0.002814 || training accuracy 100.00% || lr 2.6218487394957984e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf7883e0ed74524a6500c5385d1507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.80%, loss: 0.4928|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  11.79 min\n",
      "\n",
      "Epoch[11/15](20/192) || training loss 0.006685 || training accuracy 99.53% || lr 2.5142857142857142e-06\n",
      "Epoch[11/15](40/192) || training loss 0.001531 || training accuracy 100.00% || lr 2.4470588235294118e-06\n",
      "Epoch[11/15](60/192) || training loss 0.002856 || training accuracy 99.84% || lr 2.3798319327731093e-06\n",
      "Epoch[11/15](80/192) || training loss 0.004037 || training accuracy 99.84% || lr 2.312605042016807e-06\n",
      "Epoch[11/15](100/192) || training loss 0.003251 || training accuracy 100.00% || lr 2.2453781512605043e-06\n",
      "Epoch[11/15](120/192) || training loss 0.007113 || training accuracy 99.84% || lr 2.1781512605042014e-06\n",
      "Epoch[11/15](140/192) || training loss 0.001861 || training accuracy 100.00% || lr 2.110924369747899e-06\n",
      "Epoch[11/15](160/192) || training loss 0.001811 || training accuracy 100.00% || lr 2.0436974789915965e-06\n",
      "Epoch[11/15](180/192) || training loss 0.001926 || training accuracy 100.00% || lr 1.976470588235294e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139242c988fb488195159e6ac47e71cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.80%, loss: 0.4789|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  12.82 min\n",
      "\n",
      "Epoch[12/15](20/192) || training loss 0.002779 || training accuracy 99.84% || lr 1.86890756302521e-06\n",
      "Epoch[12/15](40/192) || training loss 0.00307 || training accuracy 100.00% || lr 1.8016806722689076e-06\n",
      "Epoch[12/15](60/192) || training loss 0.003593 || training accuracy 99.84% || lr 1.734453781512605e-06\n",
      "Epoch[12/15](80/192) || training loss 0.003506 || training accuracy 100.00% || lr 1.6672268907563026e-06\n",
      "Epoch[12/15](100/192) || training loss 0.00425 || training accuracy 100.00% || lr 1.6e-06\n",
      "Epoch[12/15](120/192) || training loss 0.001967 || training accuracy 100.00% || lr 1.5327731092436974e-06\n",
      "Epoch[12/15](140/192) || training loss 0.004414 || training accuracy 99.84% || lr 1.4655462184873948e-06\n",
      "Epoch[12/15](160/192) || training loss 0.002093 || training accuracy 100.00% || lr 1.3983193277310923e-06\n",
      "Epoch[12/15](180/192) || training loss 0.005751 || training accuracy 99.84% || lr 1.3310924369747898e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c676344a0dc94952b9234873de66386c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.40%, loss: 0.479|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  13.85 min\n",
      "\n",
      "Epoch[13/15](20/192) || training loss 0.001776 || training accuracy 100.00% || lr 1.2235294117647059e-06\n",
      "Epoch[13/15](40/192) || training loss 0.001028 || training accuracy 100.00% || lr 1.1563025210084034e-06\n",
      "Epoch[13/15](60/192) || training loss 0.002657 || training accuracy 100.00% || lr 1.0890756302521007e-06\n",
      "Epoch[13/15](80/192) || training loss 0.0009199 || training accuracy 100.00% || lr 1.0218487394957982e-06\n",
      "Epoch[13/15](100/192) || training loss 0.004904 || training accuracy 99.84% || lr 9.546218487394957e-07\n",
      "Epoch[13/15](120/192) || training loss 0.001452 || training accuracy 100.00% || lr 8.873949579831932e-07\n",
      "Epoch[13/15](140/192) || training loss 0.002073 || training accuracy 100.00% || lr 8.201680672268907e-07\n",
      "Epoch[13/15](160/192) || training loss 0.002645 || training accuracy 100.00% || lr 7.529411764705882e-07\n",
      "Epoch[13/15](180/192) || training loss 0.00177 || training accuracy 100.00% || lr 6.857142857142857e-07\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fe06e1941041d4b2b668d6eafa9af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.00%, loss: 0.4957|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  14.88 min\n",
      "\n",
      "Epoch[14/15](20/192) || training loss 0.0006829 || training accuracy 100.00% || lr 5.781512605042017e-07\n",
      "Epoch[14/15](40/192) || training loss 0.005888 || training accuracy 99.84% || lr 5.109243697478991e-07\n",
      "Epoch[14/15](60/192) || training loss 0.004824 || training accuracy 99.69% || lr 4.436974789915966e-07\n",
      "Epoch[14/15](80/192) || training loss 0.001067 || training accuracy 100.00% || lr 3.764705882352941e-07\n",
      "Epoch[14/15](100/192) || training loss 0.003102 || training accuracy 100.00% || lr 3.0924369747899157e-07\n",
      "Epoch[14/15](120/192) || training loss 0.001379 || training accuracy 100.00% || lr 2.4201680672268904e-07\n",
      "Epoch[14/15](140/192) || training loss 0.001097 || training accuracy 100.00% || lr 1.7478991596638653e-07\n",
      "Epoch[14/15](160/192) || training loss 0.002399 || training accuracy 100.00% || lr 1.0756302521008403e-07\n",
      "Epoch[14/15](180/192) || training loss 0.0005987 || training accuracy 100.00% || lr 4.033613445378151e-08\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf93bbc06045c7876a9c294c17a5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.00%, loss: 0.4968|| best acc : 92.00%, best loss: 0.2792\n",
      "Time elapsed:  15.91 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(args.model_dir, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8889d4",
   "metadata": {},
   "source": [
    "## Inference : \n",
    "---\n",
    "- target_dir(Best Val Accuracy model) 를 상황에 맞게 수정해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d3114bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_dir = \"copa_data_results/results/TrainAll_Roberta_8e-06/1/best\"\n",
    "model_module = eval(args.model_type)\n",
    "model = model_module.from_pretrained(target_dir, args=args)\n",
    "model.parameters\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238efbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "dataset = load_data(valid_data)\n",
    "test_label = dataset[\"label\"].values\n",
    "\n",
    "tokenized_test = tokenized_dataset(dataset, tokenizer, check_arch(args.model_type))\n",
    "test_dataset = CustomDataset(tokenized_test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8314d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size=8, shuffle=False)\n",
    "    model.eval()\n",
    "    results = []\n",
    "    preds = []\n",
    "\n",
    "    for i, items in enumerate(tqdm(dataloader)):\n",
    "        item = {key: val.to(device) for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**item)\n",
    "        logits = outputs[0]\n",
    "        m = nn.Softmax(dim=1)\n",
    "        logits = m(logits)\n",
    "        logits = logits.detach().cpu().numpy()  # (Batch_size, 5)  5개의 클래스 확률형태\n",
    "        pred = logits[:, 1]\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "        results += result.tolist()\n",
    "        preds += pred.tolist()\n",
    "\n",
    "    return np.array(results).flatten(), np.array(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462c7498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1964b96cd7a4551862dd950f42b57eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_answer, preds = inference(model, tokenized_sent=test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5e840ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make json\n",
    "submission_json = {\"copa\": []}\n",
    "for i, pred in enumerate(pred_answer.tolist()):\n",
    "    submission_json[\"copa\"].append({\"idx\": i, \"label\": int(pred + 1)})\n",
    "with open(\"submission.json\", \"w\") as fp:\n",
    "    json.dump(submission_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc4bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"model_answer\"] = pred_answer\n",
    "dataset[\"model_pred\"] = preds\n",
    "dataset.to_csv(\"copa_result.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d3fb2",
   "metadata": {},
   "source": [
    "# 여기까지 COPA (인과추론)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6521e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동형이의어 task 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pretrained model : klue/Roberta-large\n",
    "- R-BERT : pretrained model의 [CLS], entity1, entity2 부분의 embedding 값을 concat하여 최종 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, RobertaPreTrainedModel\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x) \n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class R_RoBERTa_WiC(RobertaPreTrainedModel):\n",
    "    def __init__(self,  model_name, config, dropout_rate):\n",
    "        super(R_RoBERTa_WiC, self).__init__(config)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer1 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer2 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "\n",
    "        self.label_classifier = FCLayer(\n",
    "            config.hidden_size * 3,\n",
    "            config.num_labels,\n",
    "            dropout_rate,\n",
    "            use_activation=False,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        \"\"\"\n",
    "        Average the entity hidden state vectors (H_i ~ H_j)\n",
    "        :param hidden_output: [batch_size, j-i+1, dim]\n",
    "        :param e_mask: [batch_size, max_seq_len]\n",
    "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
    "        :return: [batch_size, dim]\n",
    "        \"\"\"\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, e1_mask, e2_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0] #batch, max_len, hidden_size  \n",
    "\n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n",
    "        sentence_representation = self.cls_fc_layer(outputs.pooler_output)\n",
    "\n",
    "        e1_h = self.entity_fc_layer1(e1_h)\n",
    "        e2_h = self.entity_fc_layer2(e2_h)\n",
    "        # Concat -> fc_layer\n",
    "        concat_h = torch.cat([sentence_representation, e1_h, e2_h], dim=-1)\n",
    "        logits = self.label_classifier(concat_h)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        # Softmax\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lodaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WICDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(dataset_dir, mode = 'train'):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t')\n",
    "    li = []\n",
    "    for s1, s2 in zip(list(dataset['SENTENCE1']), list(dataset['SENTENCE2'])):\n",
    "        li.append(s1+' '+s2)\n",
    "    dataset[\"ANSWER\"] = dataset[\"ANSWER\"].astype(int)\n",
    "    if mode == 'test':\n",
    "        dataset[\"ANSWER\"] = [0] * len(dataset)\n",
    "    return dataset\n",
    "\n",
    "def convert_sentence_to_features(train_dataset, tokenizer, max_len):\n",
    "    \n",
    "    max_seq_len=max_len\n",
    "    pad_token=tokenizer.pad_token_id\n",
    "    add_sep_token=False\n",
    "    mask_padding_with_zero=True\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_e1_mask=[]\n",
    "    all_e2_mask=[]\n",
    "    all_label=[]\n",
    "    m_len=0\n",
    "    for idx in tqdm(range(len(train_dataset))):\n",
    "        sentence = '<s>' + train_dataset['SENTENCE1'][idx][:train_dataset['start_s1'][idx]] \\\n",
    "            + ' <e1> ' + train_dataset['SENTENCE1'][idx][train_dataset['start_s1'][idx]:train_dataset['end_s1'][idx]] \\\n",
    "            + ' </e1> ' + train_dataset['SENTENCE1'][idx][train_dataset['end_s1'][idx]:] + '</s>' \\\n",
    "            + ' ' \\\n",
    "            + '<s>' + train_dataset['SENTENCE2'][idx][:train_dataset['start_s2'][idx]] \\\n",
    "            + ' <e2> ' + train_dataset['SENTENCE2'][idx][train_dataset['start_s2'][idx]:train_dataset['end_s2'][idx]] \\\n",
    "            + ' </e2> ' + train_dataset['SENTENCE2'][idx][train_dataset['end_s2'][idx]:] + '</s>'\n",
    "        \n",
    "        token = tokenizer.tokenize(sentence)\n",
    "        m_len = max(m_len, len(token))\n",
    "        e11_p = token.index(\"<e1>\")  # the start position of entity1\n",
    "        e12_p = token.index(\"</e1>\")  # the end position of entity1\n",
    "        e21_p = token.index(\"<e2>\")  # the start position of entity2\n",
    "        e22_p = token.index(\"</e2>\")  # the end position of entity2\n",
    "\n",
    "        token[e11_p] = \"$\"\n",
    "        token[e12_p] = \"$\"\n",
    "        token[e21_p] = \"#\"\n",
    "        token[e22_p] = \"#\"\n",
    "\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        special_tokens_count = 1\n",
    "\n",
    "        if len(token) < max_seq_len - special_tokens_count:\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            padding_length = max_seq_len - len(input_ids)\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "\n",
    "            e1_mask = [0] * len(attention_mask)\n",
    "            e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "            for i in range(e11_p, e12_p + 1):\n",
    "                e1_mask[i] = 1\n",
    "            for i in range(e21_p, e22_p + 1):\n",
    "                e2_mask[i] = 1\n",
    "\n",
    "            assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
    "                len(attention_mask), max_seq_len\n",
    "            )\n",
    "\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_e1_mask.append(e1_mask)\n",
    "            all_e2_mask.append(e2_mask)\n",
    "            all_label.append(train_dataset['ANSWER'][idx])\n",
    "\n",
    "    all_features = {\n",
    "        'input_ids' : torch.tensor(all_input_ids),\n",
    "        'attention_mask' : torch.tensor(all_attention_mask),\n",
    "        'e1_mask' : torch.tensor(all_e1_mask),\n",
    "        'e2_mask' : torch.tensor(all_e2_mask)\n",
    "    }  \n",
    "    return WICDataset(all_features, all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"./\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정 \n",
    "def seed_everything(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "    }\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, args, model_dir = None,train_dataset=None, dev_dataset=None, test_dataset=None,tokenizer=None):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_dir = model_dir \n",
    "        self.best_score = 0\n",
    "        self.hold_epoch = 0\n",
    "\n",
    "        self.eval_batch_size = args.eval_batch_size\n",
    "        self.train_batch_size = args.train_batch_size\n",
    "        self.max_steps = args.max_steps\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.learning_rate = args.lr\n",
    "        self.adam_epsilon= args.adam_epsilon\n",
    "        self.warmup_steps = args.warmup_steps\n",
    "        self.num_train_epochs = args.num_train_epochs\n",
    "        self.logging_steps = args.logging_steps\n",
    "        self.max_grad_norm = args.max_grad_norm\n",
    "        self.dropout_rate = args.dropout_rate\n",
    "        self.gradient_accumulation_steps = args.gradient_accumulation_steps\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            \"klue/roberta-large\",\n",
    "            num_labels = 2\n",
    "        )\n",
    "        self.model = R_RoBERTa_WiC(\n",
    "           \"klue/roberta-large\", \n",
    "            config=self.config, \n",
    "            dropout_rate = self.dropout_rate,\n",
    "        )\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        init_logger()\n",
    "        seed_everything(args.seed)\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=self.train_batch_size,\n",
    "        )\n",
    "\n",
    "        if self.max_steps > 0:\n",
    "            t_total = self.max_steps\n",
    "            self.num_train_epochs = (\n",
    "                self.max_steps // (len(train_dataloader) // self.gradient_accumulation_steps) + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.gradient_accumulation_steps * self.num_train_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.learning_rate,\n",
    "            eps=self.adam_epsilon,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "        \n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", self.num_train_epochs)\n",
    "        logger.info(\"  Total train batch size = %d\", self.train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "        logger.info(\"  Logging steps = %d\", self.logging_steps)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = tqdm(range(int(self.num_train_epochs)), desc=\"Epoch\")\n",
    "\n",
    "        for epo_step in train_iterator:\n",
    "            self.global_epo = epo_step\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(batch[t].to(self.device) for t in batch)  # GPU or CPU\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"labels\": batch[4],\n",
    "                    \"e1_mask\": batch[2],\n",
    "                    \"e2_mask\": batch[3]\n",
    "                }\n",
    "                \n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                if self.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                if self.logging_steps > 0 and global_step % self.logging_steps == 0:\n",
    "                    logger.info(\"  global steps = %d\", global_step)\n",
    "\n",
    "                if 0 < self.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "            \n",
    "            self.evaluate(\"dev\")\n",
    "            if self.hold_epoch > 4:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "                \n",
    "            if 0 < self.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "          \n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "    \n",
    "   \n",
    "    def evaluate(self, mode):\n",
    "        # We use test dataset because semeval doesn't have dev dataset\n",
    "        if mode == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == \"dev\":\n",
    "            dataset = self.dev_dataset\n",
    "        elif mode == \"train\":\n",
    "            dataset = self.train_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info('---------------------------------------------------')\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(batch[t].to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"labels\": batch[4],\n",
    "                    \"e1_mask\": batch[2],\n",
    "                    \"e2_mask\": batch[3],\n",
    "                }\n",
    "                #with torch.cuda.amp.autocast():\n",
    "                outputs = self.model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\"loss\": eval_loss}\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        result = compute_metrics(preds, out_label_ids)\n",
    "        \n",
    "        if mode == \"dev\":\n",
    "            if result['acc']>self.best_score:\n",
    "                self.save_model()\n",
    "                self.best_score = result['acc']\n",
    "                print('save new best model acc : ',str(self.best_score))\n",
    "                self.hold_epoch = 0\n",
    "            else:\n",
    "                self.hold_epoch += 1\n",
    "        \n",
    "        \n",
    "        results.update(result)\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\n",
    "        logger.info(\"---------------------------------------------------\")\n",
    "        return results\n",
    "        \n",
    "\n",
    "    def save_model(self,new_dir=None):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        if new_dir == None:\n",
    "            pass\n",
    "        else:\n",
    "            if not os.path.exists(new_dir):\n",
    "                os.makedirs(new_dir)\n",
    "            self.model_dir = new_dir\n",
    "        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        model_to_save.save_pretrained(self.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.model_dir)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 과정은 서버에서 진행하였고, ipynb에서는 중간에 중단하였습니다.\n",
    "- data augmentation은 두 문장의 순서를 바꾸어 train dataset을 2배로 증강하였고, 이는 외부에서 실행하여 파일로 저장했습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15496/15496 [00:15<00:00, 984.14it/s]\n",
      "100%|██████████| 1166/1166 [00:01<00:00, 752.67it/s]\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:08:42 - INFO - __main__ -   ***** Running training *****\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Num examples = 15496\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Num Epochs = 10\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Total train batch size = 4\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Total optimization steps = 38740\n",
      "12/06/2021 10:08:42 - INFO - __main__ -     Logging steps = 500\n",
      "\n",
      "\u001b[A/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Iteration:   1%|          | 38/3874 [00:17<28:45,  2.22it/s]\n",
      "Epoch:   0%|          | 0/10 [00:17<?, ?it/s]\n",
      "  0%|          | 0/5 [00:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-be6ac42cfec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                   model_dir = f'{BASE_DIR}/roberta_model_fold_{str(fold)}')\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{BASE_DIR}/roberta_model_final_fold_{str(fold)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8356b4579662>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update learning rate schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    " \n",
    "        \"num_train_epochs\": 10,\n",
    "        \"train_batch_size\": 4,\n",
    "        \"eval_batch_size\": 4,\n",
    "        \"max_steps\": -1,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"lr\" : 1e-5,\n",
    "        \"adam_epsilon\" : 1e-8,\n",
    "        \"weight_decay\" : 0.01,\n",
    "        \"warmup_steps\" : 64,\n",
    "        \"seed\" : 42,\n",
    "        \"logging_steps\" : 500,\n",
    "        \"max_grad_norm\" : 1.0,\n",
    "        \"gradient_accumulation_steps\" : 1,\n",
    "        \"train_data_dir\" : f\"{BASE_DIR}dataset/wic/NIKL_SKT_WiC_Train.tsv\",\n",
    "        \"dev_data_dir\" : f\"{BASE_DIR}dataset/wic/NIKL_SKT_WiC_Dev.tsv\" \n",
    "})\n",
    "\n",
    "train_dataset = load_data(args.train_data_dir)\n",
    "dev_dataset = load_data(args.dev_data_dir)\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", return_token_type_ids=False)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
    "\n",
    "concat_dataset = train_dataset\n",
    "\n",
    "def make_fold(x):\n",
    "  if x <= concat_dataset.shape[0]*0.2:\n",
    "      return 0\n",
    "  elif x > concat_dataset.shape[0]*0.2 and x <= concat_dataset.shape[0]*0.4:\n",
    "      return 1\n",
    "  elif x > concat_dataset.shape[0]*0.4 and x <= concat_dataset.shape[0]*0.6 :\n",
    "      return 2\n",
    "  elif x > concat_dataset.shape[0]*0.6 and x <= concat_dataset.shape[0]*0.8 :\n",
    "      return 3\n",
    "  else:\n",
    "      return 4\n",
    "\n",
    "concat_dataset['fold']= concat_dataset['ID'].apply(make_fold)\n",
    "concat_dataset = concat_dataset.drop(['ID', 'Target'],axis=1)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "for fold in tqdm(range(5)): \n",
    "  trn_idx = concat_dataset[concat_dataset['fold'] != fold].index\n",
    "  val_idx = concat_dataset[concat_dataset['fold'] == fold].index\n",
    "\n",
    "  half_val_len = len(val_idx)//2\n",
    "  add_trn_idx = val_idx[:half_val_len]\n",
    "\n",
    "  trn_idx.append(add_trn_idx)\n",
    "  val_idx = val_idx[half_val_len:]\n",
    "\n",
    "  train_folds = concat_dataset.loc[trn_idx].reset_index(drop=True).drop(['fold'],axis=1)\n",
    "  valid_folds = concat_dataset.loc[val_idx].reset_index(drop=True).drop(['fold'],axis=1)\n",
    "\n",
    "  train_Dataset = convert_sentence_to_features(train_dataset, tokenizer, max_len = 280)\n",
    "  valid_Dataset = convert_sentence_to_features(dev_dataset, tokenizer, max_len= 280)\n",
    "\n",
    "  trainer = Trainer(args,\n",
    "                  train_dataset=train_Dataset,\n",
    "                  dev_dataset=valid_Dataset,\n",
    "                  tokenizer =tokenizer,\n",
    "                  model_dir = f'{BASE_DIR}roberta_model_fold_{str(fold)}')\n",
    "\n",
    "  trainer.train()\n",
    "  trainer.save_model(new_dir=f'{BASE_DIR}roberta_model_final_fold_{str(fold)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import chain\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer,AutoModel, RobertaPreTrainedModel, AutoConfig, RobertaModel\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "class Roberta_WiC(RobertaPreTrainedModel):\n",
    "    def __init__(self,  model_name, config, dropout_rate):\n",
    "        super(Roberta_WiC, self).__init__(config)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)  # Load pretrained XLMRoberta\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer1 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer2 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "\n",
    "        self.label_classifier = FCLayer(\n",
    "            config.hidden_size * 3,\n",
    "            config.num_labels,\n",
    "            dropout_rate,\n",
    "            use_activation=False,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        \"\"\"\n",
    "        Average the entity hidden state vectors (H_i ~ H_j)\n",
    "        :param hidden_output: [batch_size, j-i+1, dim]\n",
    "        :param e_mask: [batch_size, max_seq_len]\n",
    "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
    "        :return: [batch_size, dim]\n",
    "        \"\"\"\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, e1_mask, e2_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  \n",
    "        sequence_output = outputs[0] \n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "\n",
    "        sentence_representation = self.cls_fc_layer(outputs.pooler_output)\n",
    "        \n",
    "        e1_h = self.entity_fc_layer1(e1_h)\n",
    "        e2_h = self.entity_fc_layer2(e2_h)\n",
    "\n",
    "        concat_h = torch.cat([sentence_representation, e1_h, e2_h], dim=-1)\n",
    "        logits = self.label_classifier(concat_h)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        # Softmax\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    \n",
    "\n",
    "def test_pred(test_dataset, eval_batch_size, model):\n",
    "    test_dataset = test_dataset\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler,batch_size=eval_batch_size)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    init_logger()\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation on %s dataset *****\", \"test\")\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "        batch = tuple(batch[t].to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": None,\n",
    "                \"e1_mask\": batch[2],\n",
    "                \"e2_mask\": batch[3],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            pred = outputs[0]\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        if preds is None:\n",
    "            preds = pred.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, pred.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    preds_label = np.argmax(preds, axis=1)\n",
    "    df = pd.DataFrame(preds, columns=['pred_0','pred_1'])\n",
    "    df['label'] = preds_label\n",
    "    preds = preds.astype(int)\n",
    "    return df \n",
    "\n",
    "\n",
    "def load_test_data(dataset_dir):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t')\n",
    "    li = []\n",
    "    for s1, s2 in zip(list(dataset['SENTENCE1']), list(dataset['SENTENCE2'])):\n",
    "        li.append(s1+' '+s2)\n",
    "    dataset[\"ANSWER\"] = dataset[\"ANSWER\"].astype(int)\n",
    "    return dataset\n",
    "\n",
    "def convert_sentence_to_features(train_dataset, tokenizer, max_len, mode='train'):\n",
    "    max_seq_len=max_len\n",
    "    pad_token=tokenizer.pad_token_id\n",
    "    add_sep_token=False\n",
    "    mask_padding_with_zero=True\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_e1_mask=[]\n",
    "    all_e2_mask=[]\n",
    "    all_label=[]\n",
    "    m_len=0\n",
    "    for idx in tqdm(range(len(train_dataset))):\n",
    "        sentence = '<s>' + train_dataset['SENTENCE1'][idx][:train_dataset['start_s1'][idx]] \\\n",
    "            + ' <e1> ' + train_dataset['SENTENCE1'][idx][train_dataset['start_s1'][idx]:train_dataset['end_s1'][idx]] \\\n",
    "            + ' </e1> ' + train_dataset['SENTENCE1'][idx][train_dataset['end_s1'][idx]:] + '</s>' \\\n",
    "            + ' ' \\\n",
    "            + '<s>' + train_dataset['SENTENCE2'][idx][:train_dataset['start_s2'][idx]] \\\n",
    "            + ' <e2> ' + train_dataset['SENTENCE2'][idx][train_dataset['start_s2'][idx]:train_dataset['end_s2'][idx]] \\\n",
    "            + ' </e2> ' + train_dataset['SENTENCE2'][idx][train_dataset['end_s2'][idx]:] + '</s>'\n",
    "\n",
    "            \n",
    "        \n",
    "        token = tokenizer.tokenize(sentence)\n",
    "        m_len = max(m_len, len(token))\n",
    "        e11_p = token.index(\"<e1>\")  # the start position of entity1\n",
    "        e12_p = token.index(\"</e1>\")  # the end position of entity1\n",
    "        e21_p = token.index(\"<e2>\")  # the start position of entity2\n",
    "        e22_p = token.index(\"</e2>\")  # the end position of entity2\n",
    "\n",
    "        token[e11_p] = \"$\"\n",
    "        token[e12_p] = \"$\"\n",
    "        token[e21_p] = \"#\"\n",
    "        token[e22_p] = \"#\"\n",
    "\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        special_tokens_count = 1\n",
    "\n",
    "        if len(token) < max_seq_len - special_tokens_count:\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            padding_length = max_seq_len - len(input_ids)\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "\n",
    "            e1_mask = [0] * len(attention_mask)\n",
    "            e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "            for i in range(e11_p, e12_p + 1):\n",
    "                e1_mask[i] = 1\n",
    "            for i in range(e21_p, e22_p + 1):\n",
    "                e2_mask[i] = 1\n",
    "\n",
    "            assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
    "                len(attention_mask), max_seq_len\n",
    "            )\n",
    "\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_e1_mask.append(e1_mask)\n",
    "            all_e2_mask.append(e2_mask)\n",
    "            all_label.append(train_dataset['ANSWER'][idx])\n",
    "\n",
    "    all_features = {\n",
    "        'input_ids' : torch.tensor(all_input_ids),\n",
    "        'attention_mask' : torch.tensor(all_attention_mask),\n",
    "        'e1_mask' : torch.tensor(all_e1_mask),\n",
    "        'e2_mask' : torch.tensor(all_e2_mask)\n",
    "    }  \n",
    "    return RE_Dataset(all_features, all_label)\n",
    "\n",
    "def softmax(sr):\n",
    "    \n",
    "    max_val = np.max(sr)\n",
    "    exp_a = np.exp(sr-max_val)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19bd773a29a4364b33e8f2bdde4c0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58042987ad5041af8a3b1f28296e740e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:09:23 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "12/06/2021 10:09:23 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1790a9b55a8c4b498b7caa1c85ed6fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=292.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:09:55 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "12/06/2021 10:09:55 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e572b1b62844329b133fb8b40b7e6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=292.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:10:28 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "12/06/2021 10:10:28 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57569d2047f470282c7bbb79602bffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=292.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:11:00 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "12/06/2021 10:11:00 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1df3029cb75496cab4c83deb15e12c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=292.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/06/2021 10:11:33 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "12/06/2021 10:11:33 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094eefdeb5744d62ba93090c848635d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=292.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================= devset acc =================\n",
      "accuracy : 0.934819897084048\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 4\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", return_token_type_ids=False)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
    "\n",
    "test_dataset = load_test_data(f\"{BASE_DIR}dataset/wic/NIKL_SKT_WiC_Dev.tsv\")\n",
    "test_Dataset = convert_sentence_to_features(test_dataset, tokenizer, max_len= 280, mode='eval')\n",
    "\n",
    "n_fold = 5\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in tqdm(range(n_fold)):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "            \"klue/roberta-large\",\n",
    "            num_labels= 2\n",
    "        )\n",
    "    model = Roberta_WiC(\n",
    "            'klue/roberta-large',\n",
    "            config= config, \n",
    "            dropout_rate = 0.1\n",
    "        )\n",
    "    model.load_state_dict(torch.load(f'{BASE_DIR}roberta_model_final_fold_'+str(fold)+'/pytorch_model.bin', map_location=device))\n",
    "    model.eval()\n",
    "    result = test_pred(test_Dataset, eval_batch_size, model)\n",
    "    result.to_csv(f'{BASE_DIR}{str(fold)}_rbt_result.csv', index=False)\n",
    "\n",
    "ensemble= pd.DataFrame()\n",
    "for fold in range(n_fold):\n",
    "    df = pd.read_csv(f'{BASE_DIR}{str(fold)}_rbt_result.csv')\n",
    "    ensemble['label'+str(fold)]= df['label']\n",
    "\n",
    "\n",
    "soft_ensemble= pd.DataFrame()\n",
    "soft_ensemble['pred_0'] = ensemble['label0']\n",
    "soft_ensemble['pred_1'] = ensemble['label0']\n",
    "soft_ensemble['pred_0'] = 0\n",
    "soft_ensemble['pred_1'] = 0\n",
    "\n",
    "for fold in range(n_fold):\n",
    "    df = pd.read_csv(f'{BASE_DIR}{str(fold)}_rbt_result.csv')\n",
    "    df= df.drop('label',axis=1)\n",
    "    df = df.apply(softmax,axis=1)\n",
    "    soft_ensemble['pred_0'] += df['pred_0']\n",
    "    soft_ensemble['pred_1'] += df['pred_1']\n",
    "\n",
    "soft_ensemble['predicted'] = [1 if p_0 < p_1 else 0 for p_0, p_1 in zip(soft_ensemble['pred_0'], soft_ensemble['pred_1'])]\n",
    "result = compute_metrics(soft_ensemble['predicted'], test_dataset['ANSWER'])\n",
    "print('================= devset acc =================')\n",
    "print(f\"accuracy : {result['acc']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동형이의어 task 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoLA (문법성 판단)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import re\n",
    "\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch additional data\n",
    "추가 학습을 위해 KorSTS와 KorNLI 데이터셋에서 텍스트만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가적으로 사용할 추가 데이터 가져오기\n",
    "def create_additional_text(name):\n",
    "    path = Path('.')\n",
    "    root_dir = path / 'dataset'\n",
    "    Korpora.fetch(name, root_dir = root_dir)\n",
    "    corpus = Korpora.load(name)\n",
    "    additional_text = corpus.get_all_texts()\n",
    "\n",
    "    temp = []\n",
    "    for i, text in enumerate(tqdm(additional_text)):\n",
    "        temp.append(text+'\\n')\n",
    "\n",
    "    additional_text = set(temp)\n",
    "    del temp\n",
    "    additional_text = list(additional_text)\n",
    "    \n",
    "    new_file = root_dir / 'additional.txt'\n",
    "    with open(new_file, 'a+', encoding='utf-8') as writer:\n",
    "        writer.writelines(additional_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_additional_text(\"korsts\")\n",
    "#create_additional_text(\"kornli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. 추가 데이터셋에서 일부러 조사가 틀린 텍스트를 만들어서 문법이 틀린 데이터도 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyjosa import josa, jonsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SabotageSentence(object):\n",
    "    def __init__(self, sentence: str):\n",
    "        self.sentence = sentence\n",
    "        self.josa_dict = {\n",
    "            'for_jongsung':['을','은','이','과'], \n",
    "            'no_jongsung':['를','는','가','와','나','로','야','랑','며']\n",
    "        }\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_all_josa(self):\n",
    "        return self.josa_dict\n",
    "\n",
    "\n",
    "    def jongsung_wrong_josa(self) -> str:\n",
    "        new_sent = ''\n",
    "        for _, word in enumerate(self.sentence.split()):\n",
    "            tmp_word = word[:-1]\n",
    "            if word[-1] in self.josa_dict['for_jongsung']:\n",
    "                tmp_word+=random.choice(self.josa_dict['no_jongsung'])\n",
    "                new_sent+=tmp_word\n",
    "                new_sent+=' '\n",
    "            elif word[-1] in self.josa_dict['no_jongsung']:\n",
    "                tmp_word+=random.choice(self.josa_dict['for_jongsung'])\n",
    "                new_sent+=tmp_word\n",
    "                new_sent+=' '\n",
    "            else:\n",
    "                new_sent+=word\n",
    "                new_sent+=' '\n",
    "\n",
    "        return new_sent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 기존 데이터셋을 pandas DataFrame으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data_dir = Path(\"./dataset/cola/\") / filename\n",
    "    dataset = pd.read_csv(\n",
    "        data_dir, \n",
    "        sep=\"\\t\", \n",
    "        header=0, \n",
    "        encoding='utf-8', \n",
    "        names=['source', 'acceptability_label', 'source_annotation', 'sentence']\n",
    "    )\n",
    "    dataset['label'] = dataset['acceptability_label'].astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def augment_data_orig(new_data: List[str]):\n",
    "    tmp_data_holder = {'source':[], 'label':[], 'source_annotation':[], 'sentence':[]}\n",
    "    for i, row in enumerate(new_data):\n",
    "        if (re.match('[a-zA-Z]', row) is not None) or (len(row) >= 70) or (len(row) == 0) or (row[-2:]!='.\\n'):\n",
    "            continue\n",
    "        else:\n",
    "            tmp_data_holder['source'].append('T'+str(10001+i))\n",
    "            tmp_data_holder['label'].append(1)\n",
    "            tmp_data_holder['source_annotation'].append('*')\n",
    "            assert type(row) == str\n",
    "            tmp_data_holder['sentence'].append(row.replace('\\n',''))\n",
    "\n",
    "    dataset = pd.DataFrame(tmp_data_holder)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def augment_data(data):\n",
    "    tmp_data_holder = {'source':[], 'label':[], 'source_annotation':[], 'sentence':[]}\n",
    "    for _, row in data.iterrows():\n",
    "        tmp_data_holder['source'].append(row['source'])\n",
    "        tmp_data_holder['label'].append(0)\n",
    "        tmp_data_holder['source_annotation'].append(np.NaN)\n",
    "        \n",
    "        text = SabotageSentence(row['sentence']).jongsung_wrong_josa()\n",
    "        tmp_data_holder['sentence'].append(text)\n",
    "\n",
    "    dataset = pd.DataFrame(tmp_data_holder)\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "def read_txt(path='./dataset/additional.txt') -> List[str]:\n",
    "    with open(path, 'r+', encoding='utf-8') as reader:\n",
    "        new_data = reader.readlines()\n",
    "    \n",
    "    tmp_list = []\n",
    "    for text in new_data:\n",
    "        text.rstrip('\\n')\n",
    "        text.replace('\\n','')\n",
    "        tmp_list.append(text)\n",
    "    new_data = tmp_list\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def multiprocess_aug(orig_dataset, func_name):\n",
    "    num_process = multiprocessing.cpu_count()\n",
    "    \n",
    "    chunk_size = int(orig_dataset.shape[0] / num_process)\n",
    "    chunks = [orig_dataset.iloc[orig_dataset.index[i:i+chunk_size]] for i in range(0, orig_dataset.shape[0], chunk_size)]\n",
    "    assert len(chunks) != 0\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_process) as pool:\n",
    "        results = pool.map(func_name, chunks)\n",
    "        \n",
    "        new_dataset = pd.concat(results)\n",
    "        dataset = pd.concat([orig_dataset, new_dataset])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_datasets(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['sentence'].tolist()\n",
    "    tokenize_sent = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 200,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    return tokenize_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels= None, test=False):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Electra(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Electra, self).__init__(config)\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        output = outputs[0][:, 0, :]\n",
    "        output = self.linear(self.dropout(output))\n",
    "        output = torch.tanh(output)\n",
    "        logits = self.classifier(output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "  archs = {\n",
    "    \"encoder\" : [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "    \"encoder-decoder\" : [\"T5\", \"Bart\", \"Bart_BoolQ\"]\n",
    "  }\n",
    "  for arch in archs:\n",
    "    if model_type in archs[arch]:\n",
    "      return arch\n",
    "  raise ValueError(f\"Model [{model_type}] no defined archtecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def output_dir(output_path, exist_ok = False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" %path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model_dir = args.model_dir\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # tokenizer\n",
    "    MODEL_NAME = args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    datasets_ = load_data(\"./NIKL_CoLA_train.tsv\")\n",
    "\n",
    "    # 아래 코드는 데이터를 증강하는 코드지만 MCC에 도움이 되지 않는 관계로 주석처리함\n",
    "    # new_data = read_txt()\n",
    "    # new_data = augment_data_orig(new_data)\n",
    "    # new_data_corrupt = multiprocess_aug(new_data, augment_data)\n",
    "    # datasets_ = pd.concat([datasets_, new_data, new_data_corrupt], ignore_index=True)\n",
    "\n",
    "    # make validation sets from training set\n",
    "    labels_ = datasets_[\"label\"]\n",
    "    length = len(labels_)\n",
    "    kf = args.kfold\n",
    "    class_indexs = defaultdict(list)\n",
    "    for i, label_ in enumerate(labels_):\n",
    "        class_indexs[np.argmax(label_)].append(i)\n",
    "    val_indices = set()\n",
    "    for index in class_indexs: \n",
    "        val_indices = (val_indices | set(class_indexs[index][int(\n",
    "            len(class_indexs[index])*(kf-1)/9): int(len(class_indexs[index])*kf/9)]))\n",
    "    train_indices = set(range(length)) - val_indices\n",
    "\n",
    "    train_dataset = datasets_.loc[np.array(list(train_indices))]\n",
    "    val_dataset = datasets_.loc[np.array(list(val_indices))]\n",
    "\n",
    "    train_label = train_dataset['label'].values\n",
    "    val_label = val_dataset['label'].values\n",
    "\n",
    "    tokenized_train = tokenize_datasets(\n",
    "        train_dataset, tokenizer, check_arch(args.model_type))\n",
    "    tokenized_val = tokenize_datasets(\n",
    "        val_dataset, tokenizer, check_arch(args.model_type))\n",
    "\n",
    "    train_dataset = ColaDataset(tokenized_train, train_label)\n",
    "    val_dataset = ColaDataset(tokenized_val, val_label)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    config_module = getattr(import_module(\n",
    "        \"transformers\"), args.model_type + \"Config\")\n",
    "\n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model = Electra.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    save_dir = output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if ('cls_fc_layer' not in name) and ('label_classifier' not in name):  # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion = create_criterion(args.criterion)  # default: cross_entropy\n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=len(train_loader) * args.epochs,\n",
    "        last_epoch=- 1\n",
    "    )\n",
    "\n",
    "    # logging\n",
    "    best_val_mcc = -1\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(train_loader, dynamic_ncols=True)\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item['labels'])\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item['labels']).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                pbar.set_description(\n",
    "                    f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, dynamic_ncols=True)\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        acc_okay = 0\n",
    "        count_all = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        eps = 1e-9\n",
    "        for idx, items in enumerate(pbar):\n",
    "            sleep(0.01)\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            outs = model(**item)\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "            loss = criterion(outs[0], item['labels']).item()\n",
    "\n",
    "            acc_item = (item['labels'] == preds).sum().item()\n",
    "\n",
    "            TRUE = (item['labels'] == preds)\n",
    "            FALSE = (item['labels'] != preds)\n",
    "\n",
    "            TP += (TRUE * preds).sum().item()\n",
    "            TN += (TRUE * (preds == 0)).sum().item()\n",
    "            FP += (FALSE * preds).sum().item()\n",
    "            FN += (FALSE * (preds == 0)).sum().item()\n",
    "\n",
    "            val_loss_items.append(loss)\n",
    "            val_acc_items.append(acc_item)\n",
    "            acc_okay += acc_item\n",
    "            count_all += len(preds)\n",
    "\n",
    "            # Calculate MCC\n",
    "            MCC = ((TP*TN) - (FP*FN)) / \\\n",
    "                (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(val_loader)}) || val_loss: {loss:4.4} || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "        val_acc = acc_okay / count_all\n",
    "\n",
    "        if MCC > best_val_mcc:\n",
    "            print(\n",
    "                f\"New best model for val mcc : {MCC:4.2%}! saving the best model..\")\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "            torch.save(args, os.path.join(\n",
    "                f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "            best_val_mcc = MCC\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "            f\"best mcc : {best_val_mcc:4.2%}, best loss: {best_val_loss:4.4}|| \"\n",
    "            f\"MCC : {MCC:4.2%}|| \"\n",
    "            f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\"\n",
    "        )\n",
    "\n",
    "    time.sleep(5)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "args = argparse.Namespace(\n",
    "    seed = 42,\n",
    "    epochs = 30,\n",
    "    freeze_epoch=0,\n",
    "    optimizer = 'AdamW',\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = 500,\n",
    "    log_interval = 20,\n",
    "    kfold = 9,\n",
    "\n",
    "    criterion = 'cross_entropy',\n",
    "    dropout_rate = 0.1,\n",
    "    model_type = \"Electra\",\n",
    "    pretrained_model = \"tunib/electra-ko-base\",\n",
    "    lr = 4e-6,\n",
    "    batch_size = 32,\n",
    "    valid_batch_size = 128,\n",
    "\n",
    "    val_ratio=0.2,\n",
    "    name = 'exp',\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR', './results'),\n",
    "    custompretrain = \"\"\n",
    ")\n",
    "\n",
    "args.name = f'{args.model_type}_{args.lr}_{args.kfold}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "k-fold num : 9\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tunib/electra-ko-base were not used when initializing Electra: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing Electra from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Electra from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Electra were not initialized from the model checkpoint at tunib/electra-ko-base and are newly initialized: ['classifier.bias', 'linear.bias', 'classifier.weight', 'linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: [0/30](440/441) || loss: 0.5605 || acc: 74.53% || lr 3.52e-06: 100%|██████████| 441/441 [01:57<00:00,  3.74it/s]\n",
      "Epoch: [1/30](440/441) || loss: 0.4917 || acc: 78.44% || lr 3.88e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [2/30](440/441) || loss: 0.4534 || acc: 80.31% || lr 3.742e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [3/30](440/441) || loss: 0.4011 || acc: 82.66% || lr 3.603e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [4/30](440/441) || loss: 0.3474 || acc: 86.41% || lr 3.465e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [5/30](440/441) || loss: 0.3023 || acc: 87.19% || lr 3.326e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [6/30](440/441) || loss: 0.3168 || acc: 88.12% || lr 3.187e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [7/30](440/441) || loss: 0.2279 || acc: 91.41% || lr 3.049e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [8/30](440/441) || loss: 0.258 || acc: 89.53% || lr 2.91e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [9/30](440/441) || loss: 0.1729 || acc: 92.50% || lr 2.772e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [10/30](440/441) || loss: 0.1754 || acc: 92.03% || lr 2.633e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [11/30](440/441) || loss: 0.16 || acc: 93.91% || lr 2.495e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [12/30](440/441) || loss: 0.1762 || acc: 94.84% || lr 2.356e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [13/30](440/441) || loss: 0.1419 || acc: 95.00% || lr 2.217e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [14/30](440/441) || loss: 0.1195 || acc: 95.62% || lr 2.079e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [15/30](440/441) || loss: 0.1356 || acc: 94.84% || lr 1.94e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [16/30](440/441) || loss: 0.1306 || acc: 95.94% || lr 1.802e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [17/30](440/441) || loss: 0.08656 || acc: 96.72% || lr 1.663e-06: 100%|██████████| 441/441 [01:51<00:00,  3.95it/s]\n",
      "Epoch: [18/30](440/441) || loss: 0.08457 || acc: 96.41% || lr 1.525e-06: 100%|██████████| 441/441 [01:51<00:00,  3.97it/s]\n",
      "Epoch: [19/30](440/441) || loss: 0.09354 || acc: 96.56% || lr 1.386e-06: 100%|██████████| 441/441 [01:51<00:00,  3.96it/s]\n",
      "Epoch: [20/30](440/441) || loss: 0.07041 || acc: 97.66% || lr 1.247e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [21/30](440/441) || loss: 0.08624 || acc: 96.41% || lr 1.109e-06: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [22/30](440/441) || loss: 0.06864 || acc: 97.50% || lr 9.703e-07: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [23/30](440/441) || loss: 0.05669 || acc: 98.12% || lr 8.317e-07: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [24/30](440/441) || loss: 0.06478 || acc: 97.97% || lr 6.932e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [25/30](440/441) || loss: 0.09108 || acc: 97.03% || lr 5.546e-07: 100%|██████████| 441/441 [01:52<00:00,  3.93it/s]\n",
      "Epoch: [26/30](440/441) || loss: 0.04714 || acc: 98.59% || lr 4.16e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [27/30](440/441) || loss: 0.04634 || acc: 98.12% || lr 2.775e-07: 100%|██████████| 441/441 [01:52<00:00,  3.94it/s]\n",
      "Epoch: [28/30](440/441) || loss: 0.06097 || acc: 98.28% || lr 1.389e-07: 100%|██████████| 441/441 [01:52<00:00,  3.93it/s]\n",
      "Epoch: [29/30](440/441) || loss: 0.05658 || acc: 97.97% || lr 3.142e-10: 100%|██████████| 441/441 [01:51<00:00,  3.94it/s]\n",
      "Epoch: [29/30](1/14) || val_loss: 0.8176 || acc: 83.59% || MCC: 67.18%:   7%|▋         | 1/14 [00:00<00:01,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [29/30](14/14) || val_loss: 0.7995 || acc: 80.67% || MCC: 61.83%: 100%|██████████| 14/14 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val mcc : 61.83%! saving the best model..\n",
      "[Val] acc : 80.67%, loss: 0.8838|| best mcc : 61.83%, best loss: 0.8838|| MCC : 61.83%|| TP:779 / TN:644 / FP:229 / FN:112\n"
     ]
    }
   ],
   "source": [
    "print('='*40)\n",
    "print(f\"k-fold num : {args.kfold}\")\n",
    "print('='*40)\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "    file = 'NIKL_CoLA_dev.tsv'\n",
    "    dataset = load_data(file)\n",
    "    tokenized_test = tokenize_datasets(dataset, tokenizer)\n",
    "    test_label = dataset['label'].values\n",
    "    test_dataset = ColaDataset(tokenized_test, test_label)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = Electra.from_pretrained(args.model_dir) \n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(test_loader)\n",
    "    print(\"Calculating validation results...\")\n",
    "    test_acc_items = []\n",
    "    acc_okay = 0\n",
    "    count_all = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    eps = 1e-9\n",
    "\n",
    "    for idx, items in enumerate(pbar):\n",
    "        sleep(0.01)\n",
    "\n",
    "        item = {key: val.to(device) for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outs = model(**item)\n",
    "\n",
    "        preds = torch.argmax(outs[0], dim=-1)\n",
    "        labels = item['labels']\n",
    "\n",
    "        acc_item = (labels == preds).sum().item()\n",
    "\n",
    "        TRUE = (labels == preds)\n",
    "        FALSE = (labels != preds)\n",
    "\n",
    "        TP += (TRUE * preds).sum().item()\n",
    "        TN += (TRUE * (preds==0)).sum().item()\n",
    "        FP += (FALSE * preds).sum().item()\n",
    "        FN += (FALSE * (preds==0)).sum().item()\n",
    "\n",
    "        MCC = ((TP*TN) - (FP*FN)) / (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "\n",
    "        test_acc_items.append(acc_item)\n",
    "        acc_okay += acc_item\n",
    "        count_all += len(preds)\n",
    "\n",
    "        pbar.set_description(f\"({idx + 1}/{len(test_loader)}) || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "    test_acc = acc_okay / count_all\n",
    "\n",
    "    print(\n",
    "        f\"[Test] acc : {test_acc:4.2%}|| \"\n",
    "        f\"MCC : {MCC:4.2%}|| \"\n",
    "        f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\\n\"\n",
    "        f\"======================================\\n\"\n",
    "        f\"Test MCC: {MCC:4.2%}\"\n",
    "    )\n",
    "    time.sleep(5)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval args\n",
    "args = argparse.Namespace(\n",
    "    model_type = \"Electra\",\n",
    "    pretrained_model = \"tunib/electra-ko-base\",\n",
    "\n",
    "    model_dir = './results/Electra_4e-06_9/97/best',\n",
    "    criterion = 'cross_entropy',\n",
    "    num_labels=2,\n",
    "\n",
    "    test_batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(9/254) || acc: 76.39% || MCC: 53.29%:   2%|▏         | 5/254 [00:00<00:05, 49.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(254/254) || acc: 75.84% || MCC: 51.64%: 100%|██████████| 254/254 [00:05<00:00, 48.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] acc : 75.84%|| MCC : 51.64%|| TP:892 / TN:649 / FP:312 / FN:179\n",
      "======================================\n",
      "Test MCC: 51.64%\n"
     ]
    }
   ],
   "source": [
    "evaluate(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "BoolQ.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13a2f4649e8047a8afeaf03f59d33279": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6ddc2206f25455a8fcabbdb4ca07b74",
       "IPY_MODEL_5b074748a57e4cbab17fa772b48e58b4"
      ],
      "layout": "IPY_MODEL_81d37d49671547e6b5cf5f9796a7c341"
     }
    },
    "5b074748a57e4cbab17fa772b48e58b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bf7d1a4e2b04682b2f1a08eb8b2ecd5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e73ff60e42c54fa4a436d301f1e77159",
      "value": 1
     }
    },
    "73c019de2e974d9d981fc14c7dad42f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf7d1a4e2b04682b2f1a08eb8b2ecd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81d37d49671547e6b5cf5f9796a7c341": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abe820ab8fb4420b90c0cad00f1bd870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6ddc2206f25455a8fcabbdb4ca07b74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73c019de2e974d9d981fc14c7dad42f7",
      "placeholder": "​",
      "style": "IPY_MODEL_abe820ab8fb4420b90c0cad00f1bd870",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "e73ff60e42c54fa4a436d301f1e77159": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
