{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df718ea",
   "metadata": {},
   "source": [
    "# COPA 학습 & Inference to json 코드\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed96a7",
   "metadata": {},
   "source": [
    "## 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd52003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from time import sleep\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    ElectraModel,\n",
    "    ElectraPreTrainedModel,\n",
    "    XLMRobertaModel,\n",
    "    BartModel,\n",
    "    BartPretrainedModel,\n",
    "    T5Model,\n",
    "    RobertaModel,\n",
    ")\n",
    "from transformers import MBartModel, MBartConfig\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3856b",
   "metadata": {},
   "source": [
    "## Transformers의 Wrapper Class와 일부 테스트 모델 선언 및 구현부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcef7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, inner_dim: int, pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Bert(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Bert, self).__init__(config)\n",
    "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class XLMRoberta(XLMRobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(XLMRoberta, self).__init__(config)\n",
    "        self.xlmroberta = XLMRobertaModel.from_pretrained(\n",
    "            \"xlm-roberta-large\", config=config\n",
    "        )  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.xlmroberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.xlmroberta(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class Electra_BoolQ(ElectraPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Electra_BoolQ, self).__init__(config)\n",
    "\n",
    "        # self.num_labels = config.num_labels\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = ElectraModel.from_pretrained(\n",
    "            \"monologg/koelectra-base-v3-discriminator\", config=config\n",
    "        )\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "        # self.sparse = Sparsemax()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.model(\n",
    "            input_ids2, attention_mask=attention_mask2, token_type_ids=token_type_ids2\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class Roberta(RobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            \"klue/roberta-large\", config=config\n",
    "        )  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(\n",
    "            input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1,\n",
    "        )\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels - 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.roberta(input_ids2, attention_mask=attention_mask2)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "\n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[\n",
    "            2:\n",
    "        ]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a14d2",
   "metadata": {},
   "source": [
    "## 데이터 전처리부\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5845e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def load_data(dataset_dir):\n",
    "    dataset = pd.read_csv(\n",
    "        dataset_dir,\n",
    "        delimiter=\"\\t\",\n",
    "        names=[\"ID\", \"sentence\", \"question\", \"1\", \"2\", \"answer\"],\n",
    "        header=0,\n",
    "    )\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int) - 1\n",
    "\n",
    "    new_sentence1_1 = []\n",
    "    new_sentence1_2 = []\n",
    "    new_sentence2_1 = []\n",
    "    new_sentence2_2 = []\n",
    "    for i in range(len(dataset)):\n",
    "        s = dataset.iloc[i][\"sentence\"]\n",
    "        q = dataset.iloc[i][\"question\"]\n",
    "        s1 = dataset.iloc[i][\"1\"]\n",
    "        s2 = dataset.iloc[i][\"2\"]\n",
    "        lb = dataset.iloc[i][\"label\"]\n",
    "        if q == \"결과\":\n",
    "            new_sentence1_1.append(\"[결과]\" + s)\n",
    "            # new_sentence1_1.append(s)\n",
    "            new_sentence1_2.append(s1)\n",
    "            new_sentence2_1.append(\"[결과]\" + s)\n",
    "            # new_sentence2_1.append(s)\n",
    "            new_sentence2_2.append(s2)\n",
    "\n",
    "        else:\n",
    "            new_sentence1_1.append(\"[원인]\" + s1)\n",
    "            # new_sentence1_1.append(s1)\n",
    "            new_sentence1_2.append(s)\n",
    "            new_sentence2_1.append(\"[원인]\" + s2)\n",
    "            # new_sentence2_1.append(s2)\n",
    "            new_sentence2_2.append(s)\n",
    "\n",
    "    dataset[\"new_sentence1_1\"] = new_sentence1_1\n",
    "    dataset[\"new_sentence1_2\"] = new_sentence1_2\n",
    "    dataset[\"new_sentence2_1\"] = new_sentence2_1\n",
    "    dataset[\"new_sentence2_2\"] = new_sentence2_2\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence1_1 = dataset[\"new_sentence1_1\"].tolist()\n",
    "    sentence1_2 = dataset[\"new_sentence1_2\"].tolist()\n",
    "    sentence2_1 = dataset[\"new_sentence2_1\"].tolist()\n",
    "    sentence2_2 = dataset[\"new_sentence2_2\"].tolist()\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentence1_1,\n",
    "        sentence1_2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    tokenized_sentences2 = tokenizer(\n",
    "        sentence2_1,\n",
    "        sentence2_2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    for key, value in tokenized_sentences2.items():\n",
    "        tokenized_sentences[key + \"2\"] = value\n",
    "\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046a4c9",
   "metadata": {},
   "source": [
    "## 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccaa0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arch(model_type):\n",
    "    archs = {\n",
    "        \"encoder\": [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "        \"encoder-decoder\": [\"T5\", \"Bart\", \"Bart_BoolQ\"],\n",
    "    }\n",
    "    for arch in archs:\n",
    "        if model_type in archs[arch]:\n",
    "            return arch\n",
    "    raise ValueError(f\"Model [{model_type}] no defined archtecture\")\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }\n",
    "\n",
    "def increment_output_dir(output_path, exist_ok=False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\"\n",
    "\n",
    "def train(model_dir, args):\n",
    "\n",
    "    seed_everything(args.seed)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device(GPU) : {torch.cuda.is_available()}\")\n",
    "    num_classes = 2\n",
    "\n",
    "    # load model and tokenizerƒ\n",
    "    MODEL_NAME = args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_data(\"./dataset/copa/SKT_COPA_Train.tsv\")\n",
    "    val_dataset = load_data(\"./dataset/copa/SKT_COPA_Dev.tsv\")\n",
    "\n",
    "    train_label = train_dataset[\"label\"].values\n",
    "    val_label = val_dataset[\"label\"].values\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(\n",
    "        train_dataset, tokenizer, check_arch(args.model_type)\n",
    "    )\n",
    "    tokenized_val = tokenized_dataset(\n",
    "        val_dataset, tokenizer, check_arch(args.model_type)\n",
    "    )\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    train_dataset = CustomDataset(tokenized_train, train_label)\n",
    "    val_dataset = CustomDataset(tokenized_val, val_label)\n",
    "    # -- data_loader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=args.valid_batch_size, shuffle=False, drop_last=False,\n",
    "    )\n",
    "\n",
    "    # setting model hyperparameter\n",
    "    if args.model_type == \"Electra_BoolQ\":\n",
    "        config_module = ElectraConfig\n",
    "    else:\n",
    "        config_module = getattr(\n",
    "            import_module(\"transformers\"), args.model_type + \"Config\"\n",
    "        )\n",
    "\n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model_module = eval(args.model_type)\n",
    "\n",
    "    if args.model_type in [\"BERT\", \"Electra\"]:\n",
    "        model = model_module.from_pretrained(\n",
    "            MODEL_NAME, config=model_config, args=args\n",
    "        )\n",
    "    else:\n",
    "        model = model_module(config=model_config, args=args)\n",
    "\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    save_dir = increment_output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "    # Freeze Parameter\n",
    "    for name, param in model.named_parameters():\n",
    "        if (\"cls_fc_layer\" not in name) and (\n",
    "            \"label_classifier\" not in name\n",
    "        ):  # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay, eps=1e-8\n",
    "    )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=len(train_loader) * args.epochs,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "\n",
    "    # -- logging\n",
    "    start_time = time.time()\n",
    "    logger = SummaryWriter(log_dir=save_dir)\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vars(args), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        # train loop\n",
    "        # unFreeze parameters\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(train_loader):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item[\"labels\"])\n",
    "\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item[\"labels\"]).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                logger.add_scalar(\n",
    "                    \"Train/loss\", train_loss, epoch * len(train_loader) + idx\n",
    "                )\n",
    "                logger.add_scalar(\n",
    "                    \"Train/accuracy\", train_acc, epoch * len(train_loader) + idx\n",
    "                )\n",
    "                logger.add_scalar(\"LR\", current_lr, epoch * len(train_loader) + idx)\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            acc_okay = 0\n",
    "            count_all = 0\n",
    "            for idx, items in enumerate(tqdm(val_loader)):\n",
    "                sleep(0.01)\n",
    "                item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "                outs = model(**item)\n",
    "\n",
    "                preds = torch.argmax(outs[0], dim=-1)\n",
    "                loss = criterion(outs[0], item[\"labels\"]).item()\n",
    "\n",
    "                acc_item = (item[\"labels\"] == preds).sum().item()\n",
    "\n",
    "                val_loss_items.append(loss)\n",
    "                val_acc_items.append(acc_item)\n",
    "                acc_okay += acc_item\n",
    "                count_all += len(preds)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "            val_acc = acc_okay / count_all\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\n",
    "                    f\"New best model for val acc : {val_acc:4.2%}! saving the best model..\"\n",
    "                )\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.4}\"\n",
    "            )\n",
    "\n",
    "            logger.add_scalar(\"Val/loss\", val_loss, epoch)\n",
    "            logger.add_scalar(\"Val/accuracy\", val_acc, epoch)\n",
    "            s = f\"Time elapsed: {(time.time() - start_time)/60: .2f} min\"\n",
    "            print(s)\n",
    "            print()\n",
    "            if epoch > 24:\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb487ed6",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "args  = EasyDict(dict(\n",
    "    epochs = 20,\n",
    "    model_type = \"Roberta\",\n",
    "    pretrained_model = \"klue/roberta-large\",\n",
    "    lr = 8e-6,\n",
    "    batch_size = 32,\n",
    "    freeze_epoch = 0,\n",
    "    valid_batch_size = 128,\n",
    "    val_ratio = 0.2,\n",
    "    dropout_rate = 0.1,\n",
    "    criterion = 'cross_entropy',\n",
    "    optimizer = 'AdamW',\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = 500,\n",
    "    seed = 42,\n",
    "    log_interval = 20,\n",
    "    kfold = 1,\n",
    "    model_dir = \"./copa_data_results/results\",\n",
    "))\n",
    "    \n",
    "    \n",
    "    \n",
    "args.name = f'TrainAll_{args.model_type}_{args.lr}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3b9cc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cccef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(GPU) : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/20](20/96) || training loss 0.6946 || training accuracy 48.91% || lr 3.2e-07\n",
      "Epoch[0/20](40/96) || training loss 0.6953 || training accuracy 45.62% || lr 6.4e-07\n",
      "Epoch[0/20](60/96) || training loss 0.6916 || training accuracy 51.72% || lr 9.6e-07\n",
      "Epoch[0/20](80/96) || training loss 0.6929 || training accuracy 51.88% || lr 1.28e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08095d8ce5b74f94a8dbcd0f85c70051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 64.60%! saving the best model..\n",
      "[Val] acc : 64.60%, loss: 0.6916|| best acc : 64.60%, best loss: 0.6916\n",
      "Time elapsed:  0.47 min\n",
      "\n",
      "Epoch[1/20](20/96) || training loss 0.6949 || training accuracy 51.41% || lr 1.856e-06\n",
      "Epoch[1/20](40/96) || training loss 0.6866 || training accuracy 55.94% || lr 2.176e-06\n",
      "Epoch[1/20](60/96) || training loss 0.6763 || training accuracy 61.09% || lr 2.496e-06\n",
      "Epoch[1/20](80/96) || training loss 0.565 || training accuracy 74.53% || lr 2.816e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c1ab00bb24cb4b9347159ba2bdbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 86.80%! saving the best model..\n",
      "[Val] acc : 86.80%, loss: 0.3683|| best acc : 86.80%, best loss: 0.3683\n",
      "Time elapsed:  1.03 min\n",
      "\n",
      "Epoch[2/20](20/96) || training loss 0.3661 || training accuracy 84.69% || lr 3.392e-06\n",
      "Epoch[2/20](40/96) || training loss 0.3448 || training accuracy 85.62% || lr 3.712e-06\n",
      "Epoch[2/20](60/96) || training loss 0.3482 || training accuracy 85.94% || lr 4.032e-06\n",
      "Epoch[2/20](80/96) || training loss 0.2247 || training accuracy 91.41% || lr 4.352e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29216d1fa1e473b870e3ace90dde482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 90.20%! saving the best model..\n",
      "[Val] acc : 90.20%, loss: 0.2971|| best acc : 90.20%, best loss: 0.2971\n",
      "Time elapsed:  1.59 min\n",
      "\n",
      "Epoch[3/20](20/96) || training loss 0.1969 || training accuracy 91.72% || lr 4.928e-06\n",
      "Epoch[3/20](40/96) || training loss 0.1922 || training accuracy 93.28% || lr 5.248e-06\n",
      "Epoch[3/20](60/96) || training loss 0.1778 || training accuracy 93.91% || lr 5.567999999999999e-06\n",
      "Epoch[3/20](80/96) || training loss 0.1803 || training accuracy 92.03% || lr 5.887999999999999e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e29e984802b456fb21c824e59c95955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 89.60%, loss: 0.3174|| best acc : 90.20%, best loss: 0.2971\n",
      "Time elapsed:  2.00 min\n",
      "\n",
      "Epoch[4/20](20/96) || training loss 0.09187 || training accuracy 96.56% || lr 6.464e-06\n",
      "Epoch[4/20](40/96) || training loss 0.09666 || training accuracy 96.09% || lr 6.784e-06\n",
      "Epoch[4/20](60/96) || training loss 0.1046 || training accuracy 96.09% || lr 7.104e-06\n",
      "Epoch[4/20](80/96) || training loss 0.1265 || training accuracy 95.94% || lr 7.424e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d688dd7fcd4444a0a740b5eb06166b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 91.00%! saving the best model..\n",
      "[Val] acc : 91.00%, loss: 0.3339|| best acc : 91.00%, best loss: 0.2971\n",
      "Time elapsed:  2.55 min\n",
      "\n",
      "Epoch[5/20](20/96) || training loss 0.06943 || training accuracy 98.12% || lr 8e-06\n",
      "Epoch[5/20](40/96) || training loss 0.04864 || training accuracy 98.44% || lr 7.887323943661972e-06\n",
      "Epoch[5/20](60/96) || training loss 0.06444 || training accuracy 97.34% || lr 7.774647887323943e-06\n",
      "Epoch[5/20](80/96) || training loss 0.0898 || training accuracy 96.56% || lr 7.661971830985914e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c975eb76a1473a91c8f6271fa2f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model for val acc : 92.40%! saving the best model..\n",
      "[Val] acc : 92.40%, loss: 0.3091|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  3.11 min\n",
      "\n",
      "Epoch[6/20](20/96) || training loss 0.04308 || training accuracy 98.91% || lr 7.459154929577465e-06\n",
      "Epoch[6/20](40/96) || training loss 0.03518 || training accuracy 98.59% || lr 7.3464788732394365e-06\n",
      "Epoch[6/20](60/96) || training loss 0.03919 || training accuracy 98.59% || lr 7.233802816901408e-06\n",
      "Epoch[6/20](80/96) || training loss 0.03839 || training accuracy 97.81% || lr 7.12112676056338e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035617d3fb324414954d0ff8a87b636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.80%, loss: 0.376|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  3.52 min\n",
      "\n",
      "Epoch[7/20](20/96) || training loss 0.0218 || training accuracy 99.06% || lr 6.918309859154929e-06\n",
      "Epoch[7/20](40/96) || training loss 0.02091 || training accuracy 99.53% || lr 6.805633802816901e-06\n",
      "Epoch[7/20](60/96) || training loss 0.02408 || training accuracy 99.53% || lr 6.6929577464788726e-06\n",
      "Epoch[7/20](80/96) || training loss 0.03042 || training accuracy 98.75% || lr 6.580281690140845e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cdec3aebf14eee8999e19bf2d3a1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.40%, loss: 0.3941|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  3.93 min\n",
      "\n",
      "Epoch[8/20](20/96) || training loss 0.02373 || training accuracy 98.91% || lr 6.377464788732395e-06\n",
      "Epoch[8/20](40/96) || training loss 0.02151 || training accuracy 99.22% || lr 6.264788732394366e-06\n",
      "Epoch[8/20](60/96) || training loss 0.01639 || training accuracy 99.69% || lr 6.152112676056338e-06\n",
      "Epoch[8/20](80/96) || training loss 0.008522 || training accuracy 99.84% || lr 6.0394366197183095e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e236d6e6983466aa6d94196c035a4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 92.40%, loss: 0.4694|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  4.34 min\n",
      "\n",
      "Epoch[9/20](20/96) || training loss 0.01163 || training accuracy 99.53% || lr 5.836619718309859e-06\n",
      "Epoch[9/20](40/96) || training loss 0.01632 || training accuracy 99.69% || lr 5.723943661971831e-06\n",
      "Epoch[9/20](60/96) || training loss 0.01219 || training accuracy 99.53% || lr 5.611267605633802e-06\n",
      "Epoch[9/20](80/96) || training loss 0.0234 || training accuracy 99.53% || lr 5.4985915492957745e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5464fd568218493a8b30aa5fa8549aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 92.00%, loss: 0.4444|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  4.75 min\n",
      "\n",
      "Epoch[10/20](20/96) || training loss 0.02596 || training accuracy 98.91% || lr 5.295774647887324e-06\n",
      "Epoch[10/20](40/96) || training loss 0.0148 || training accuracy 99.69% || lr 5.183098591549296e-06\n",
      "Epoch[10/20](60/96) || training loss 0.01284 || training accuracy 99.53% || lr 5.070422535211268e-06\n",
      "Epoch[10/20](80/96) || training loss 0.01002 || training accuracy 99.69% || lr 4.957746478873239e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e0927632334a2f996c37aff8318f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 92.40%, loss: 0.4539|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  5.17 min\n",
      "\n",
      "Epoch[11/20](20/96) || training loss 0.0116 || training accuracy 99.53% || lr 4.754929577464788e-06\n",
      "Epoch[11/20](40/96) || training loss 0.005938 || training accuracy 100.00% || lr 4.64225352112676e-06\n",
      "Epoch[11/20](60/96) || training loss 0.01036 || training accuracy 99.69% || lr 4.529577464788733e-06\n",
      "Epoch[11/20](80/96) || training loss 0.006183 || training accuracy 99.84% || lr 4.4169014084507046e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ecf18018714ae49d24fe866994f00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 90.80%, loss: 0.4617|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  5.58 min\n",
      "\n",
      "Epoch[12/20](20/96) || training loss 0.01075 || training accuracy 99.69% || lr 4.214084507042253e-06\n",
      "Epoch[12/20](40/96) || training loss 0.005227 || training accuracy 99.84% || lr 4.101408450704225e-06\n",
      "Epoch[12/20](60/96) || training loss 0.002887 || training accuracy 100.00% || lr 3.988732394366197e-06\n",
      "Epoch[12/20](80/96) || training loss 0.005085 || training accuracy 100.00% || lr 3.876056338028169e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba8331d530742c8873c17da4ec43796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.80%, loss: 0.4453|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  5.99 min\n",
      "\n",
      "Epoch[13/20](20/96) || training loss 0.002965 || training accuracy 100.00% || lr 3.6732394366197183e-06\n",
      "Epoch[13/20](40/96) || training loss 0.004252 || training accuracy 100.00% || lr 3.56056338028169e-06\n",
      "Epoch[13/20](60/96) || training loss 0.008777 || training accuracy 99.84% || lr 3.4478873239436615e-06\n",
      "Epoch[13/20](80/96) || training loss 0.00348 || training accuracy 100.00% || lr 3.335211267605634e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0568c76958bd45b58a458c78d96e1bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.60%, loss: 0.4442|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  6.40 min\n",
      "\n",
      "Epoch[14/20](20/96) || training loss 0.004145 || training accuracy 100.00% || lr 3.132394366197183e-06\n",
      "Epoch[14/20](40/96) || training loss 0.003527 || training accuracy 99.84% || lr 3.0197183098591547e-06\n",
      "Epoch[14/20](60/96) || training loss 0.002808 || training accuracy 100.00% || lr 2.9070422535211266e-06\n",
      "Epoch[14/20](80/96) || training loss 0.003978 || training accuracy 100.00% || lr 2.7943661971830984e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eab8ead98b541a08f7caa0885ca8b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.40%, loss: 0.453|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  6.82 min\n",
      "\n",
      "Epoch[15/20](20/96) || training loss 0.001923 || training accuracy 100.00% || lr 2.591549295774648e-06\n",
      "Epoch[15/20](40/96) || training loss 0.003019 || training accuracy 100.00% || lr 2.4788732394366193e-06\n",
      "Epoch[15/20](60/96) || training loss 0.002429 || training accuracy 100.00% || lr 2.366197183098591e-06\n",
      "Epoch[15/20](80/96) || training loss 0.002802 || training accuracy 100.00% || lr 2.2535211267605635e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf3718369a41299a8694c4225a2153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.60%, loss: 0.4582|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  7.23 min\n",
      "\n",
      "Epoch[16/20](20/96) || training loss 0.002695 || training accuracy 100.00% || lr 2.0507042253521125e-06\n",
      "Epoch[16/20](40/96) || training loss 0.00178 || training accuracy 100.00% || lr 1.9380281690140844e-06\n",
      "Epoch[16/20](60/96) || training loss 0.00179 || training accuracy 100.00% || lr 1.8253521126760562e-06\n",
      "Epoch[16/20](80/96) || training loss 0.004201 || training accuracy 100.00% || lr 1.712676056338028e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fe91063560411ea553f39a0a23b50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.60%, loss: 0.4638|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  7.64 min\n",
      "\n",
      "Epoch[17/20](20/96) || training loss 0.004792 || training accuracy 99.84% || lr 1.5098591549295774e-06\n",
      "Epoch[17/20](40/96) || training loss 0.001833 || training accuracy 100.00% || lr 1.3971830985915492e-06\n",
      "Epoch[17/20](60/96) || training loss 0.002725 || training accuracy 100.00% || lr 1.284507042253521e-06\n",
      "Epoch[17/20](80/96) || training loss 0.001229 || training accuracy 100.00% || lr 1.171830985915493e-06\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b758f88741743799f8c50168a82e9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.60%, loss: 0.4605|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  8.06 min\n",
      "\n",
      "Epoch[18/20](20/96) || training loss 0.002797 || training accuracy 100.00% || lr 9.690140845070422e-07\n",
      "Epoch[18/20](40/96) || training loss 0.002013 || training accuracy 100.00% || lr 8.56338028169014e-07\n",
      "Epoch[18/20](60/96) || training loss 0.002531 || training accuracy 99.84% || lr 7.436619718309859e-07\n",
      "Epoch[18/20](80/96) || training loss 0.002906 || training accuracy 99.84% || lr 6.309859154929577e-07\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a832bcd209f4c3c9ec927e96c89d02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 92.00%, loss: 0.464|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  8.47 min\n",
      "\n",
      "Epoch[19/20](20/96) || training loss 0.0007484 || training accuracy 100.00% || lr 4.28169014084507e-07\n",
      "Epoch[19/20](40/96) || training loss 0.004334 || training accuracy 99.84% || lr 3.1549295774647887e-07\n",
      "Epoch[19/20](60/96) || training loss 0.003066 || training accuracy 99.84% || lr 2.028169014084507e-07\n",
      "Epoch[19/20](80/96) || training loss 0.003256 || training accuracy 99.69% || lr 9.014084507042254e-08\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9717f5e51cb24cb0ab390a5703eb8248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] acc : 91.60%, loss: 0.4652|| best acc : 92.40%, best loss: 0.2971\n",
      "Time elapsed:  8.88 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(args.model_dir, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8889d4",
   "metadata": {},
   "source": [
    "## Inference : \n",
    "---\n",
    "- target_dir(Best Val Accuracy model) 를 상황에 맞게 수정해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3114bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_dir = \"copa_data_results/results/TrainAll_Roberta_8e-06/1/best\"\n",
    "model_module = eval(args.model_type)\n",
    "model = model_module.from_pretrained(target_dir, args=args)\n",
    "model.parameters\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238efbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "dataset = load_data(\"dataset/copa/SKT_COPA_Dev.tsv\")\n",
    "test_label = dataset[\"label\"].values\n",
    "\n",
    "tokenized_test = tokenized_dataset(dataset, tokenizer, check_arch(args.model_type))\n",
    "test_dataset = CustomDataset(tokenized_test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8314d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size=8, shuffle=False)\n",
    "    model.eval()\n",
    "    results = []\n",
    "    preds = []\n",
    "\n",
    "    for i, items in enumerate(tqdm(dataloader)):\n",
    "        item = {key: val.to(device) for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**item)\n",
    "        logits = outputs[0]\n",
    "        m = nn.Softmax(dim=1)\n",
    "        logits = m(logits)\n",
    "        logits = logits.detach().cpu().numpy()  # (Batch_size, 5)  5개의 클래스 확률형태\n",
    "        pred = logits[:, 1]\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "        results += result.tolist()\n",
    "        preds += pred.tolist()\n",
    "\n",
    "    return np.array(results).flatten(), np.array(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462c7498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233c2e0c9ed048a3a5963fe943ef6b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_answer, preds = inference(model, tokenized_sent=test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e840ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make json\n",
    "submission_json = {\"copa\": []}\n",
    "for i, pred in enumerate(pred_answer.tolist()):\n",
    "    submission_json[\"copa\"].append({\"idx\": i, \"label\": int(pred + 1)})\n",
    "with open(\"submission.json\", \"w\") as fp:\n",
    "    json.dump(submission_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc4bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"model_answer\"] = pred_answer\n",
    "dataset[\"model_pred\"] = preds\n",
    "dataset.to_csv(\"copa_result.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d0b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
